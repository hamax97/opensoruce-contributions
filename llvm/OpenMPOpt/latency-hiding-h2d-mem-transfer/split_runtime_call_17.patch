diff --git a/llvm/include/llvm/Frontend/OpenMP/OMPKinds.def b/llvm/include/llvm/Frontend/OpenMP/OMPKinds.def
index f286403e657..6f2ecb3f388 100644
--- a/llvm/include/llvm/Frontend/OpenMP/OMPKinds.def
+++ b/llvm/include/llvm/Frontend/OpenMP/OMPKinds.def
@@ -1,1149 +1,1153 @@
 //===--- OMPKinds.def - OpenMP directives, clauses, rt-calls -*- C++ -*-===//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
 // See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
 //===----------------------------------------------------------------------===//
 /// \file
 ///
 /// This file defines the list of supported OpenMP directives, clauses, runtime
 /// calls, and other things that need to be listed in enums.
 ///
 //===----------------------------------------------------------------------===//
 
 /// OpenMP Directives and combined directives
 ///
 ///{
 
 #ifndef OMP_DIRECTIVE
 #define OMP_DIRECTIVE(Enum, Str)
 #endif
 
 #define __OMP_DIRECTIVE_EXT(Name, Str) OMP_DIRECTIVE(OMPD_##Name, Str)
 #define __OMP_DIRECTIVE(Name) __OMP_DIRECTIVE_EXT(Name, #Name)
 
 __OMP_DIRECTIVE(threadprivate)
 __OMP_DIRECTIVE(parallel)
 __OMP_DIRECTIVE(task)
 __OMP_DIRECTIVE(simd)
 __OMP_DIRECTIVE(for)
 __OMP_DIRECTIVE(sections)
 __OMP_DIRECTIVE(section)
 __OMP_DIRECTIVE(single)
 __OMP_DIRECTIVE(master)
 __OMP_DIRECTIVE(critical)
 __OMP_DIRECTIVE(taskyield)
 __OMP_DIRECTIVE(barrier)
 __OMP_DIRECTIVE(taskwait)
 __OMP_DIRECTIVE(taskgroup)
 __OMP_DIRECTIVE(flush)
 __OMP_DIRECTIVE(ordered)
 __OMP_DIRECTIVE(atomic)
 __OMP_DIRECTIVE(target)
 __OMP_DIRECTIVE(teams)
 __OMP_DIRECTIVE(cancel)
 __OMP_DIRECTIVE(requires)
 __OMP_DIRECTIVE_EXT(target_data, "target data")
 __OMP_DIRECTIVE_EXT(target_enter_data, "target enter data")
 __OMP_DIRECTIVE_EXT(target_exit_data, "target exit data")
 __OMP_DIRECTIVE_EXT(target_parallel, "target parallel")
 __OMP_DIRECTIVE_EXT(target_parallel_for, "target parallel for")
 __OMP_DIRECTIVE_EXT(target_update, "target update")
 __OMP_DIRECTIVE_EXT(parallel_for, "parallel for")
 __OMP_DIRECTIVE_EXT(parallel_for_simd, "parallel for simd")
 __OMP_DIRECTIVE_EXT(parallel_master, "parallel master")
 __OMP_DIRECTIVE_EXT(parallel_sections, "parallel sections")
 __OMP_DIRECTIVE_EXT(for_simd, "for simd")
 __OMP_DIRECTIVE_EXT(cancellation_point, "cancellation point")
 __OMP_DIRECTIVE_EXT(declare_reduction, "declare reduction")
 __OMP_DIRECTIVE_EXT(declare_mapper, "declare mapper")
 __OMP_DIRECTIVE_EXT(declare_simd, "declare simd")
 __OMP_DIRECTIVE(taskloop)
 __OMP_DIRECTIVE_EXT(taskloop_simd, "taskloop simd")
 __OMP_DIRECTIVE(distribute)
 __OMP_DIRECTIVE_EXT(declare_target, "declare target")
 __OMP_DIRECTIVE_EXT(end_declare_target, "end declare target")
 __OMP_DIRECTIVE_EXT(distribute_parallel_for, "distribute parallel for")
 __OMP_DIRECTIVE_EXT(distribute_parallel_for_simd,
                     "distribute parallel for simd")
 __OMP_DIRECTIVE_EXT(distribute_simd, "distribute simd")
 __OMP_DIRECTIVE_EXT(target_parallel_for_simd, "target parallel for simd")
 __OMP_DIRECTIVE_EXT(target_simd, "target simd")
 __OMP_DIRECTIVE_EXT(teams_distribute, "teams distribute")
 __OMP_DIRECTIVE_EXT(teams_distribute_simd, "teams distribute simd")
 __OMP_DIRECTIVE_EXT(teams_distribute_parallel_for_simd,
                     "teams distribute parallel for simd")
 __OMP_DIRECTIVE_EXT(teams_distribute_parallel_for,
                     "teams distribute parallel for")
 __OMP_DIRECTIVE_EXT(target_teams, "target teams")
 __OMP_DIRECTIVE_EXT(target_teams_distribute, "target teams distribute")
 __OMP_DIRECTIVE_EXT(target_teams_distribute_parallel_for,
                     "target teams distribute parallel for")
 __OMP_DIRECTIVE_EXT(target_teams_distribute_parallel_for_simd,
                     "target teams distribute parallel for simd")
 __OMP_DIRECTIVE_EXT(target_teams_distribute_simd,
                     "target teams distribute simd")
 __OMP_DIRECTIVE(allocate)
 __OMP_DIRECTIVE_EXT(declare_variant, "declare variant")
 __OMP_DIRECTIVE_EXT(master_taskloop, "master taskloop")
 __OMP_DIRECTIVE_EXT(parallel_master_taskloop, "parallel master taskloop")
 __OMP_DIRECTIVE_EXT(master_taskloop_simd, "master taskloop simd")
 __OMP_DIRECTIVE_EXT(parallel_master_taskloop_simd,
                     "parallel master taskloop simd")
 __OMP_DIRECTIVE(depobj)
 __OMP_DIRECTIVE(scan)
 __OMP_DIRECTIVE_EXT(begin_declare_variant, "begin declare variant")
 __OMP_DIRECTIVE_EXT(end_declare_variant, "end declare variant")
 
 // Has to be the last because Clang implicitly expects it to be.
 __OMP_DIRECTIVE(unknown)
 
 #undef __OMP_DIRECTIVE_EXT
 #undef __OMP_DIRECTIVE
 #undef OMP_DIRECTIVE
 
 ///}
 
 /// OpenMP Clauses
 ///
 ///{
 
 #ifndef OMP_CLAUSE
 #define OMP_CLAUSE(Enum, Str, Implicit)
 #endif
 #ifndef OMP_CLAUSE_CLASS
 #define OMP_CLAUSE_CLASS(Enum, Str, Class)
 #endif
 #ifndef OMP_CLAUSE_NO_CLASS
 #define OMP_CLAUSE_NO_CLASS(Enum, Str)
 #endif
 
 #define __OMP_CLAUSE(Name, Class)                                              \
   OMP_CLAUSE(OMPC_##Name, #Name, /* Implicit */ false)                         \
   OMP_CLAUSE_CLASS(OMPC_##Name, #Name, Class)
 #define __OMP_CLAUSE_NO_CLASS(Name)                                            \
   OMP_CLAUSE(OMPC_##Name, #Name, /* Implicit */ false)                         \
   OMP_CLAUSE_NO_CLASS(OMPC_##Name, #Name)
 #define __OMP_IMPLICIT_CLAUSE_CLASS(Name, Str, Class)                          \
   OMP_CLAUSE(OMPC_##Name, Str, /* Implicit */ true)                            \
   OMP_CLAUSE_CLASS(OMPC_##Name, Str, Class)
 #define __OMP_IMPLICIT_CLAUSE_NO_CLASS(Name, Str)                              \
   OMP_CLAUSE(OMPC_##Name, Str, /* Implicit */ true)                            \
   OMP_CLAUSE_NO_CLASS(OMPC_##Name, Str)
 
 __OMP_CLAUSE(allocator, OMPAllocatorClause)
 __OMP_CLAUSE(if, OMPIfClause)
 __OMP_CLAUSE(final, OMPFinalClause)
 __OMP_CLAUSE(num_threads, OMPNumThreadsClause)
 __OMP_CLAUSE(safelen, OMPSafelenClause)
 __OMP_CLAUSE(simdlen, OMPSimdlenClause)
 __OMP_CLAUSE(collapse, OMPCollapseClause)
 __OMP_CLAUSE(default, OMPDefaultClause)
 __OMP_CLAUSE(private, OMPPrivateClause)
 __OMP_CLAUSE(firstprivate, OMPFirstprivateClause)
 __OMP_CLAUSE(lastprivate, OMPLastprivateClause)
 __OMP_CLAUSE(shared, OMPSharedClause)
 __OMP_CLAUSE(reduction, OMPReductionClause)
 __OMP_CLAUSE(linear, OMPLinearClause)
 __OMP_CLAUSE(aligned, OMPAlignedClause)
 __OMP_CLAUSE(copyin, OMPCopyinClause)
 __OMP_CLAUSE(copyprivate, OMPCopyprivateClause)
 __OMP_CLAUSE(proc_bind, OMPProcBindClause)
 __OMP_CLAUSE(schedule, OMPScheduleClause)
 __OMP_CLAUSE(ordered, OMPOrderedClause)
 __OMP_CLAUSE(nowait, OMPNowaitClause)
 __OMP_CLAUSE(untied, OMPUntiedClause)
 __OMP_CLAUSE(mergeable, OMPMergeableClause)
 __OMP_CLAUSE(read, OMPReadClause)
 __OMP_CLAUSE(write, OMPWriteClause)
 __OMP_CLAUSE(update, OMPUpdateClause)
 __OMP_CLAUSE(capture, OMPCaptureClause)
 __OMP_CLAUSE(seq_cst, OMPSeqCstClause)
 __OMP_CLAUSE(acq_rel, OMPAcqRelClause)
 __OMP_CLAUSE(acquire, OMPAcquireClause)
 __OMP_CLAUSE(release, OMPReleaseClause)
 __OMP_CLAUSE(relaxed, OMPRelaxedClause)
 __OMP_CLAUSE(depend, OMPDependClause)
 __OMP_CLAUSE(device, OMPDeviceClause)
 __OMP_CLAUSE(threads, OMPThreadsClause)
 __OMP_CLAUSE(simd, OMPSIMDClause)
 __OMP_CLAUSE(map, OMPMapClause)
 __OMP_CLAUSE(num_teams, OMPNumTeamsClause)
 __OMP_CLAUSE(thread_limit, OMPThreadLimitClause)
 __OMP_CLAUSE(priority, OMPPriorityClause)
 __OMP_CLAUSE(grainsize, OMPGrainsizeClause)
 __OMP_CLAUSE(nogroup, OMPNogroupClause)
 __OMP_CLAUSE(num_tasks, OMPNumTasksClause)
 __OMP_CLAUSE(hint, OMPHintClause)
 __OMP_CLAUSE(dist_schedule, OMPDistScheduleClause)
 __OMP_CLAUSE(defaultmap, OMPDefaultmapClause)
 __OMP_CLAUSE(to, OMPToClause)
 __OMP_CLAUSE(from, OMPFromClause)
 __OMP_CLAUSE(use_device_ptr, OMPUseDevicePtrClause)
 __OMP_CLAUSE(is_device_ptr, OMPIsDevicePtrClause)
 __OMP_CLAUSE(task_reduction, OMPTaskReductionClause)
 __OMP_CLAUSE(in_reduction, OMPInReductionClause)
 __OMP_CLAUSE(unified_address, OMPUnifiedAddressClause)
 __OMP_CLAUSE(unified_shared_memory, OMPUnifiedSharedMemoryClause)
 __OMP_CLAUSE(reverse_offload, OMPReverseOffloadClause)
 __OMP_CLAUSE(dynamic_allocators, OMPDynamicAllocatorsClause)
 __OMP_CLAUSE(atomic_default_mem_order, OMPAtomicDefaultMemOrderClause)
 __OMP_CLAUSE(allocate, OMPAllocateClause)
 __OMP_CLAUSE(nontemporal, OMPNontemporalClause)
 __OMP_CLAUSE(order, OMPOrderClause)
 __OMP_CLAUSE(destroy, OMPDestroyClause)
 __OMP_CLAUSE(detach, OMPDetachClause)
 __OMP_CLAUSE(inclusive, OMPInclusiveClause)
 __OMP_CLAUSE(exclusive, OMPExclusiveClause)
 __OMP_CLAUSE(uses_allocators, OMPUsesAllocatorsClause)
 __OMP_CLAUSE(affinity, OMPAffinityClause)
 __OMP_CLAUSE(use_device_addr, OMPUseDeviceAddrClause)
 
 __OMP_CLAUSE_NO_CLASS(uniform)
 __OMP_CLAUSE_NO_CLASS(device_type)
 __OMP_CLAUSE_NO_CLASS(match)
 
 __OMP_IMPLICIT_CLAUSE_CLASS(depobj, "depobj", OMPDepobjClause)
 __OMP_IMPLICIT_CLAUSE_CLASS(flush, "flush", OMPFlushClause)
 
 __OMP_IMPLICIT_CLAUSE_NO_CLASS(threadprivate, "threadprivate or thread local")
 __OMP_IMPLICIT_CLAUSE_NO_CLASS(unknown, "unknown")
 
 #undef __OMP_IMPLICIT_CLAUSE_NO_CLASS
 #undef __OMP_IMPLICIT_CLAUSE_CLASS
 #undef __OMP_CLAUSE
 #undef OMP_CLAUSE_NO_CLASS
 #undef OMP_CLAUSE_CLASS
 #undef OMP_CLAUSE
 
 ///}
 
 /// Types used in runtime structs or runtime functions
 ///
 ///{
 
 #ifndef OMP_TYPE
 #define OMP_TYPE(VarName, InitValue)
 #endif
 
 #define __OMP_TYPE(VarName) OMP_TYPE(VarName, Type::get##VarName##Ty(Ctx))
 
 __OMP_TYPE(Void)
 __OMP_TYPE(Int1)
 __OMP_TYPE(Int8)
 __OMP_TYPE(Int32)
 __OMP_TYPE(Int64)
 __OMP_TYPE(Int8Ptr)
 __OMP_TYPE(Int32Ptr)
 __OMP_TYPE(Int64Ptr)
 
 OMP_TYPE(SizeTy, M.getDataLayout().getIntPtrType(Ctx))
 
 #define __OMP_PTR_TYPE(NAME, BASE) OMP_TYPE(NAME, BASE->getPointerTo())
 
 __OMP_PTR_TYPE(VoidPtr, Int8)
 __OMP_PTR_TYPE(VoidPtrPtr, VoidPtr)
 __OMP_PTR_TYPE(VoidPtrPtrPtr, VoidPtrPtr)
 
 __OMP_PTR_TYPE(Int8PtrPtr, Int8Ptr)
 __OMP_PTR_TYPE(Int8PtrPtrPtr, Int8PtrPtr)
 
 #undef __OMP_PTR_TYPE
 
 #undef __OMP_TYPE
 #undef OMP_TYPE
 
 ///}
 
 /// array types
 ///
 ///{
 
 #ifndef OMP_ARRAY_TYPE
 #define OMP_ARRAY_TYPE(VarName, ElemTy, ArraySize)
 #endif
 
 #define __OMP_ARRAY_TYPE(VarName, ElemTy, ArraySize)                           \
   OMP_ARRAY_TYPE(VarName, ElemTy, ArraySize)
 
 __OMP_ARRAY_TYPE(KmpCriticalName, Int32, 8)
 
 #undef __OMP_ARRAY_TYPE
 #undef OMP_ARRAY_TYPE
 
 ///}
 
 /// Struct and function types
 ///
 ///{
 
 #ifndef OMP_STRUCT_TYPE
 #define OMP_STRUCT_TYPE(VarName, StructName, ...)
 #endif
 
 #define __OMP_STRUCT_TYPE(VarName, Name, ...)                                  \
   OMP_STRUCT_TYPE(VarName, "struct." #Name, __VA_ARGS__)
 
 __OMP_STRUCT_TYPE(Ident, ident_t, Int32, Int32, Int32, Int32, Int8Ptr)
+__OMP_STRUCT_TYPE(AsyncInfo, __tgt_async_info, Int8Ptr)
 
 #undef __OMP_STRUCT_TYPE
 #undef OMP_STRUCT_TYPE
 
 #ifndef OMP_FUNCTION_TYPE
 #define OMP_FUNCTION_TYPE(VarName, IsVarArg, ReturnType, ...)
 #endif
 
 #define __OMP_FUNCTION_TYPE(VarName, IsVarArg, ReturnType, ...)                \
   OMP_FUNCTION_TYPE(VarName, IsVarArg, ReturnType, __VA_ARGS__)
 
 __OMP_FUNCTION_TYPE(ParallelTask, true, Void, Int32Ptr, Int32Ptr)
 __OMP_FUNCTION_TYPE(ReduceFunction, false, Void, VoidPtr, VoidPtr)
 __OMP_FUNCTION_TYPE(CopyFunction, false, Void, VoidPtr, VoidPtr)
 __OMP_FUNCTION_TYPE(KmpcCtor, false, VoidPtr, VoidPtr)
 __OMP_FUNCTION_TYPE(KmpcDtor, false, Void, VoidPtr)
 __OMP_FUNCTION_TYPE(KmpcCopyCtor, false, VoidPtr, VoidPtr, VoidPtr)
 __OMP_FUNCTION_TYPE(TaskRoutineEntry, false, Int32, Int32,
                     /* kmp_task_t */ VoidPtr)
 
 #undef __OMP_FUNCTION_TYPE
 #undef OMP_FUNCTION_TYPE
 
 ///}
 
 /// Internal Control Variables information
 ///
 ///{
 
 #ifndef ICV_DATA_ENV
 #define ICV_DATA_ENV(Enum, Name, EnvVarName, Init)
 #endif
 
 #define __ICV_DATA_ENV(Name, EnvVarName, Init)                                 \
   ICV_DATA_ENV(ICV_##Name, #Name, #EnvVarName, Init)
 
 __ICV_DATA_ENV(nthreads, OMP_NUM_THREADS, ICV_IMPLEMENTATION_DEFINED)
 __ICV_DATA_ENV(active_levels, NONE, ICV_ZERO)
 __ICV_DATA_ENV(cancel, OMP_CANCELLATION, ICV_FALSE)
 __ICV_DATA_ENV(__last, last, ICV_LAST)
 
 #undef __ICV_DATA_ENV
 #undef ICV_DATA_ENV
 
 #ifndef ICV_RT_SET
 #define ICV_RT_SET(Name, RTL)
 #endif
 
 #define __ICV_RT_SET(Name, RTL) ICV_RT_SET(ICV_##Name, OMPRTL_##RTL)
 
 __ICV_RT_SET(nthreads, omp_set_num_threads)
 
 #undef __ICV_RT_SET
 #undef ICV_RT_SET
 
 #ifndef ICV_RT_GET
 #define ICV_RT_GET(Name, RTL)
 #endif
 
 #define __ICV_RT_GET(Name, RTL) ICV_RT_GET(ICV_##Name, OMPRTL_##RTL)
 
 __ICV_RT_GET(nthreads, omp_get_max_threads)
 __ICV_RT_GET(active_levels, omp_get_active_level)
 __ICV_RT_GET(cancel, omp_get_cancellation)
 
 #undef __ICV_RT_GET
 #undef ICV_RT_GET
 
 ///}
 
 /// Runtime library function (and their attributes)
 ///
 ///{
 
 #ifndef OMP_RTL
 #define OMP_RTL(Enum, Str, IsVarArg, ReturnType, ...)
 #endif
 
 #define __OMP_RTL(Name, IsVarArg, ReturnType, ...)                             \
   OMP_RTL(OMPRTL_##Name, #Name, IsVarArg, ReturnType, __VA_ARGS__)
 
 
 
 __OMP_RTL(__kmpc_barrier, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_cancel, false, Int32, IdentPtr, Int32, Int32)
 __OMP_RTL(__kmpc_cancel_barrier, false, Int32, IdentPtr, Int32)
 __OMP_RTL(__kmpc_flush, false, Void, IdentPtr)
 __OMP_RTL(__kmpc_global_thread_num, false, Int32, IdentPtr)
 __OMP_RTL(__kmpc_fork_call, true, Void, IdentPtr, Int32, ParallelTaskPtr)
 __OMP_RTL(__kmpc_omp_taskwait, false, Int32, IdentPtr, Int32)
 __OMP_RTL(__kmpc_omp_taskyield, false, Int32, IdentPtr, Int32, /* Int */ Int32)
 __OMP_RTL(__kmpc_push_num_threads, false, Void, IdentPtr, Int32,
           /* Int */ Int32)
 __OMP_RTL(__kmpc_push_proc_bind, false, Void, IdentPtr, Int32, /* Int */ Int32)
 __OMP_RTL(__kmpc_serialized_parallel, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_end_serialized_parallel, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_omp_reg_task_with_affinity, false, Int32, IdentPtr, Int32,
           Int8Ptr, Int32, Int8Ptr)
 
 __OMP_RTL(omp_get_thread_num, false, Int32, )
 __OMP_RTL(omp_get_num_threads, false, Int32, )
 __OMP_RTL(omp_get_max_threads, false, Int32, )
 __OMP_RTL(omp_in_parallel, false, Int32, )
 __OMP_RTL(omp_get_dynamic, false, Int32, )
 __OMP_RTL(omp_get_cancellation, false, Int32, )
 __OMP_RTL(omp_get_nested, false, Int32, )
 __OMP_RTL(omp_get_schedule, false, Void, Int32Ptr, Int32Ptr)
 __OMP_RTL(omp_get_thread_limit, false, Int32, )
 __OMP_RTL(omp_get_supported_active_levels, false, Int32, )
 __OMP_RTL(omp_get_max_active_levels, false, Int32, )
 __OMP_RTL(omp_get_level, false, Int32, )
 __OMP_RTL(omp_get_ancestor_thread_num, false, Int32, Int32)
 __OMP_RTL(omp_get_team_size, false, Int32, Int32)
 __OMP_RTL(omp_get_active_level, false, Int32, )
 __OMP_RTL(omp_in_final, false, Int32, )
 __OMP_RTL(omp_get_proc_bind, false, Int32, )
 __OMP_RTL(omp_get_num_places, false, Int32, )
 __OMP_RTL(omp_get_num_procs, false, Int32, )
 __OMP_RTL(omp_get_place_proc_ids, false, Void, Int32, Int32Ptr)
 __OMP_RTL(omp_get_place_num, false, Int32, )
 __OMP_RTL(omp_get_partition_num_places, false, Int32, )
 __OMP_RTL(omp_get_partition_place_nums, false, Void, Int32Ptr)
 
 __OMP_RTL(omp_set_num_threads, false, Void, Int32)
 __OMP_RTL(omp_set_dynamic, false, Void, Int32)
 __OMP_RTL(omp_set_nested, false, Void, Int32)
 __OMP_RTL(omp_set_schedule, false, Void, Int32, Int32)
 __OMP_RTL(omp_set_max_active_levels, false, Void, Int32)
 
 __OMP_RTL(__kmpc_master, false, Int32, IdentPtr, Int32)
 __OMP_RTL(__kmpc_end_master, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_critical, false, Void, IdentPtr, Int32, KmpCriticalNamePtrTy)
 __OMP_RTL(__kmpc_critical_with_hint, false, Void, IdentPtr, Int32,
           KmpCriticalNamePtrTy, Int32)
 __OMP_RTL(__kmpc_end_critical, false, Void, IdentPtr, Int32,
           KmpCriticalNamePtrTy)
 
 __OMP_RTL(__kmpc_begin, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_end, false, Void, IdentPtr)
 
 __OMP_RTL(__kmpc_reduce, false, Int32, IdentPtr, Int32, Int32, SizeTy, VoidPtr,
           ReduceFunctionPtr, KmpCriticalNamePtrTy)
 __OMP_RTL(__kmpc_reduce_nowait, false, Int32, IdentPtr, Int32, Int32, SizeTy,
           VoidPtr, ReduceFunctionPtr, KmpCriticalNamePtrTy)
 __OMP_RTL(__kmpc_end_reduce, false, Void, IdentPtr, Int32,
           KmpCriticalNamePtrTy)
 __OMP_RTL(__kmpc_end_reduce_nowait, false, Void, IdentPtr, Int32,
           KmpCriticalNamePtrTy)
 
 __OMP_RTL(__kmpc_ordered, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_end_ordered, false, Void, IdentPtr, Int32)
 
 __OMP_RTL(__kmpc_for_static_init_4, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int32Ptr, Int32Ptr, Int32Ptr, Int32, Int32)
 __OMP_RTL(__kmpc_for_static_init_4u, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int32Ptr, Int32Ptr, Int32Ptr, Int32, Int32)
 __OMP_RTL(__kmpc_for_static_init_8, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int64Ptr, Int64Ptr, Int64Ptr, Int64, Int64)
 __OMP_RTL(__kmpc_for_static_init_8u, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int64Ptr, Int64Ptr, Int64Ptr, Int64, Int64)
 __OMP_RTL(__kmpc_for_static_fini, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_dist_dispatch_init_4, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int32, Int32, Int32, Int32)
 __OMP_RTL(__kmpc_dist_dispatch_init_4u, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int32, Int32, Int32, Int32)
 __OMP_RTL(__kmpc_dist_dispatch_init_8, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int64, Int64, Int64, Int64)
 __OMP_RTL(__kmpc_dist_dispatch_init_8u, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int64, Int64, Int64, Int64)
 __OMP_RTL(__kmpc_dispatch_init_4, false, Void, IdentPtr, Int32, Int32, Int32,
           Int32, Int32, Int32)
 __OMP_RTL(__kmpc_dispatch_init_4u, false, Void, IdentPtr, Int32, Int32, Int32,
           Int32, Int32, Int32)
 __OMP_RTL(__kmpc_dispatch_init_8, false, Void, IdentPtr, Int32, Int32, Int64,
           Int64, Int64, Int64)
 __OMP_RTL(__kmpc_dispatch_init_8u, false, Void, IdentPtr, Int32, Int32, Int64,
           Int64, Int64, Int64)
 __OMP_RTL(__kmpc_dispatch_next_4, false, Int32, IdentPtr, Int32, Int32Ptr,
           Int32Ptr, Int32Ptr, Int32Ptr)
 __OMP_RTL(__kmpc_dispatch_next_4u, false, Int32, IdentPtr, Int32, Int32Ptr,
           Int32Ptr, Int32Ptr, Int32Ptr)
 __OMP_RTL(__kmpc_dispatch_next_8, false, Int32, IdentPtr, Int32, Int32Ptr,
           Int64Ptr, Int64Ptr, Int64Ptr)
 __OMP_RTL(__kmpc_dispatch_next_8u, false, Int32, IdentPtr, Int32, Int32Ptr,
           Int64Ptr, Int64Ptr, Int64Ptr)
 __OMP_RTL(__kmpc_dispatch_fini_4, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_dispatch_fini_4u, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_dispatch_fini_8, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_dispatch_fini_8u, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_team_static_init_4, false, Void, IdentPtr, Int32, Int32Ptr,
           Int32Ptr, Int32Ptr, Int32Ptr, Int32, Int32)
 __OMP_RTL(__kmpc_team_static_init_4u, false, Void, IdentPtr, Int32, Int32Ptr,
           Int32Ptr, Int32Ptr, Int32Ptr, Int32, Int32)
 __OMP_RTL(__kmpc_team_static_init_8, false, Void, IdentPtr, Int32, Int32Ptr,
           Int64Ptr, Int64Ptr, Int64Ptr, Int64, Int64)
 __OMP_RTL(__kmpc_team_static_init_8u, false, Void, IdentPtr, Int32, Int32Ptr,
           Int64Ptr, Int64Ptr, Int64Ptr, Int64, Int64)
 __OMP_RTL(__kmpc_dist_for_static_init_4, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int32Ptr, Int32Ptr, Int32Ptr, Int32Ptr, Int32, Int32)
 __OMP_RTL(__kmpc_dist_for_static_init_4u, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int32Ptr, Int32Ptr, Int32Ptr, Int32Ptr, Int32, Int32)
 __OMP_RTL(__kmpc_dist_for_static_init_8, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int64Ptr, Int64Ptr, Int64Ptr, Int64Ptr, Int64, Int64)
 __OMP_RTL(__kmpc_dist_for_static_init_8u, false, Void, IdentPtr, Int32, Int32,
           Int32Ptr, Int64Ptr, Int64Ptr, Int64Ptr, Int64Ptr, Int64, Int64)
 
 __OMP_RTL(__kmpc_single, false, Int32, IdentPtr, Int32)
 __OMP_RTL(__kmpc_end_single, false, Void, IdentPtr, Int32)
 
 __OMP_RTL(__kmpc_omp_task_alloc, false, /* kmp_task_t */ VoidPtr, IdentPtr,
           Int32, Int32, SizeTy, SizeTy, TaskRoutineEntryPtr)
 __OMP_RTL(__kmpc_omp_task, false, Int32, IdentPtr, Int32,
           /* kmp_task_t */ VoidPtr)
 __OMP_RTL(__kmpc_end_taskgroup, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_taskgroup, false, Void, IdentPtr, Int32)
 __OMP_RTL(__kmpc_omp_task_begin_if0, false, Void, IdentPtr, Int32,
           /* kmp_task_t */ VoidPtr)
 __OMP_RTL(__kmpc_omp_task_complete_if0, false, Void, IdentPtr, Int32,
           /* kmp_tasK_t */ VoidPtr)
 __OMP_RTL(__kmpc_omp_task_with_deps, false, Int32, IdentPtr, Int32,
           /* kmp_task_t */ VoidPtr, Int32,
           /* kmp_depend_info_t */ VoidPtr, Int32,
           /* kmp_depend_info_t */ VoidPtr)
 __OMP_RTL(__kmpc_taskloop, false, Void, IdentPtr, /* Int */ Int32, VoidPtr,
           /* Int */ Int32, Int64Ptr, Int64Ptr, Int64, /* Int */ Int32,
           /* Int */ Int32, Int64, VoidPtr)
 __OMP_RTL(__kmpc_omp_target_task_alloc, false, /* kmp_task_t */ VoidPtr,
           IdentPtr, Int32, Int32, SizeTy, SizeTy, TaskRoutineEntryPtr, Int64)
 __OMP_RTL(__kmpc_taskred_modifier_init, false, VoidPtr, IdentPtr,
           /* Int */ Int32, /* Int */ Int32, /* Int */ Int32, VoidPtr)
 __OMP_RTL(__kmpc_taskred_init, false, VoidPtr, /* Int */ Int32,
           /* Int */ Int32, VoidPtr)
 __OMP_RTL(__kmpc_task_reduction_modifier_fini, false, Void, IdentPtr,
           /* Int */ Int32, /* Int */ Int32)
 __OMP_RTL(__kmpc_task_reduction_get_th_data, false, VoidPtr, Int32, VoidPtr,
           VoidPtr)
 __OMP_RTL(__kmpc_task_reduction_init, false, VoidPtr, Int32, Int32, VoidPtr)
 __OMP_RTL(__kmpc_task_reduction_modifier_init, false, VoidPtr, VoidPtr, Int32,
           Int32, Int32, VoidPtr)
 __OMP_RTL(__kmpc_proxy_task_completed_ooo, false, Void, VoidPtr)
 
 __OMP_RTL(__kmpc_omp_wait_deps, false, Void, IdentPtr, Int32, Int32,
           /* kmp_depend_info_t */ VoidPtr, Int32, VoidPtr)
 __OMP_RTL(__kmpc_cancellationpoint, false, Int32, IdentPtr, Int32, Int32)
 
 __OMP_RTL(__kmpc_fork_teams, true, Void, IdentPtr, Int32, ParallelTaskPtr)
 __OMP_RTL(__kmpc_push_num_teams, false, Void, IdentPtr, Int32, Int32, Int32)
 
 __OMP_RTL(__kmpc_copyprivate, false, Void, IdentPtr, Int32, SizeTy, VoidPtr,
           CopyFunctionPtr, Int32)
 __OMP_RTL(__kmpc_threadprivate_cached, false, VoidPtr, IdentPtr, Int32, VoidPtr,
           SizeTy, VoidPtrPtrPtr)
 __OMP_RTL(__kmpc_threadprivate_register, false, Void, IdentPtr, VoidPtr,
           KmpcCtorPtr, KmpcCopyCtorPtr, KmpcDtorPtr)
 
 __OMP_RTL(__kmpc_doacross_init, false, Void, IdentPtr, Int32, Int32,
           /* kmp_dim */ VoidPtr)
 __OMP_RTL(__kmpc_doacross_post, false, Void, IdentPtr, Int32, Int64Ptr)
 __OMP_RTL(__kmpc_doacross_wait, false, Void, IdentPtr, Int32, Int64Ptr)
 __OMP_RTL(__kmpc_doacross_fini, false, Void, IdentPtr, Int32)
 
 __OMP_RTL(__kmpc_alloc, false, VoidPtr, /* Int */ Int32, SizeTy, VoidPtr)
 __OMP_RTL(__kmpc_free, false, Void, /* Int */ Int32, VoidPtr, VoidPtr)
 
 __OMP_RTL(__kmpc_init_allocator, false, /* omp_allocator_handle_t */ VoidPtr,
           /* Int */ Int32, /* omp_memespace_handle_t */ VoidPtr,
           /* Int */ Int32, /* omp_alloctrait_t */ VoidPtr)
 __OMP_RTL(__kmpc_destroy_allocator, false, Void, /* Int */ Int32,
           /* omp_allocator_handle_t */ VoidPtr)
 
 __OMP_RTL(__kmpc_push_target_tripcount, false, Void, Int64, Int64)
 __OMP_RTL(__tgt_target, false, Int32, Int64, VoidPtr, Int32, VoidPtrPtr,
           VoidPtrPtr, Int64Ptr, Int64Ptr)
 __OMP_RTL(__tgt_target_nowait, false, Int32, Int64, VoidPtr, Int32, VoidPtrPtr,
           VoidPtrPtr, Int64Ptr, Int64Ptr)
 __OMP_RTL(__tgt_target_teams, false, Int32, Int64, VoidPtr, Int32, VoidPtrPtr,
           VoidPtrPtr, Int64Ptr, Int64Ptr, Int32, Int32)
 __OMP_RTL(__tgt_target_teams_nowait, false, Int32, Int64, VoidPtr, Int32,
           VoidPtrPtr, VoidPtrPtr, Int64Ptr, Int64Ptr, Int32, Int32)
 __OMP_RTL(__tgt_register_requires, false, Void, Int64)
 __OMP_RTL(__tgt_target_data_begin, false, Void, Int64, Int32, VoidPtrPtr,
           VoidPtrPtr, Int64Ptr, Int64Ptr)
 __OMP_RTL(__tgt_target_data_begin_nowait, false, Void, Int64, Int32, VoidPtrPtr,
           VoidPtrPtr, Int64Ptr, Int64Ptr)
+__OMP_RTL(__tgt_target_data_begin_issue, false, AsyncInfo, Int64, Int32, VoidPtrPtr,
+          VoidPtrPtr, Int64Ptr, Int64Ptr)
+__OMP_RTL(__tgt_target_data_begin_wait, false, Void, Int64, AsyncInfo)
 __OMP_RTL(__tgt_target_data_end, false, Void, Int64, Int32, VoidPtrPtr,
           VoidPtrPtr, Int64Ptr, Int64Ptr)
 __OMP_RTL(__tgt_target_data_end_nowait, false, Void, Int64, Int32, VoidPtrPtr,
           VoidPtrPtr, Int64Ptr, Int64Ptr)
 __OMP_RTL(__tgt_target_data_update, false, Void, Int64, Int32, VoidPtrPtr,
           VoidPtrPtr, Int64Ptr, Int64Ptr)
 __OMP_RTL(__tgt_target_data_update_nowait, false, Void, Int64, Int32,
           VoidPtrPtr, VoidPtrPtr, Int64Ptr, Int64Ptr)
 __OMP_RTL(__tgt_mapper_num_components, false, Int64, VoidPtr)
 __OMP_RTL(__tgt_push_mapper_component, false, Void, VoidPtr, VoidPtr, VoidPtr,
           Int64, Int64)
 __OMP_RTL(__kmpc_task_allow_completion_event, false, VoidPtr, IdentPtr,
           /* Int */ Int32, /* kmp_task_t */ VoidPtr)
 
 __OMP_RTL(__last, false, Void, )
 
 #undef __OMP_RTL
 #undef OMP_RTL
 
 #define EnumAttr(Kind) Attribute::get(Ctx, Attribute::AttrKind::Kind)
 #define AttributeSet(...)                                                      \
   AttributeSet::get(Ctx, ArrayRef<Attribute>({__VA_ARGS__}))
 
 #ifndef OMP_ATTRS_SET
 #define OMP_ATTRS_SET(VarName, AttrSet)
 #endif
 
 #define __OMP_ATTRS_SET(VarName, AttrSet) OMP_ATTRS_SET(VarName, AttrSet)
 
 __OMP_ATTRS_SET(GetterAttrs,
                 OptimisticAttributes
                     ? AttributeSet(EnumAttr(NoUnwind), EnumAttr(ReadOnly),
                                    EnumAttr(NoSync), EnumAttr(NoFree), EnumAttr(InaccessibleMemOnly))
                     : AttributeSet(EnumAttr(NoUnwind)))
 __OMP_ATTRS_SET(GetterArgWriteAttrs,
                 OptimisticAttributes
                     ? AttributeSet(EnumAttr(NoUnwind), EnumAttr(NoSync),
                                    EnumAttr(NoFree), EnumAttr(InaccessibleMemOrArgMemOnly))
                     : AttributeSet(EnumAttr(NoUnwind)))
 __OMP_ATTRS_SET(SetterAttrs,
                 OptimisticAttributes
                     ? AttributeSet(EnumAttr(NoUnwind), EnumAttr(WriteOnly),
                                    EnumAttr(NoSync), EnumAttr(NoFree), EnumAttr(InaccessibleMemOnly))
                     : AttributeSet(EnumAttr(NoUnwind)))
 
 #undef __OMP_ATTRS_SET
 #undef OMP_ATTRS_SET
 
 #ifndef OMP_RTL_ATTRS
 #define OMP_RTL_ATTRS(Enum, FnAttrSet, RetAttrSet, ArgAttrSets)
 #endif
 
 #define __OMP_RTL_ATTRS(Name, FnAttrSet, RetAttrSet, ArgAttrSets)              \
   OMP_RTL_ATTRS(OMPRTL_##Name, FnAttrSet, RetAttrSet, ArgAttrSets)
 
 __OMP_RTL_ATTRS(__kmpc_barrier, AttributeSet(), AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_cancel,
                 AttributeSet(EnumAttr(InaccessibleMemOrArgMemOnly)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_cancel_barrier, AttributeSet(), AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_flush, AttributeSet(), AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_global_thread_num, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_fork_call, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_omp_taskwait, AttributeSet(), AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_omp_taskyield,
                 AttributeSet(EnumAttr(InaccessibleMemOrArgMemOnly)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_push_num_threads,
                 AttributeSet(EnumAttr(InaccessibleMemOrArgMemOnly)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_push_proc_bind,
                 AttributeSet(EnumAttr(InaccessibleMemOrArgMemOnly)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_serialized_parallel,
                 AttributeSet(EnumAttr(InaccessibleMemOrArgMemOnly)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_end_serialized_parallel,
                 AttributeSet(EnumAttr(InaccessibleMemOrArgMemOnly)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(omp_get_thread_num, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_num_threads, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_max_threads, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_in_parallel, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_dynamic, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_cancellation, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_nested, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_schedule, GetterArgWriteAttrs, AttributeSet(),
                 ArrayRef<AttributeSet>(
                     {AttributeSet(EnumAttr(NoCapture), EnumAttr(WriteOnly)),
                      AttributeSet(EnumAttr(NoCapture), EnumAttr(WriteOnly))}))
 __OMP_RTL_ATTRS(omp_get_thread_limit, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_supported_active_levels, GetterAttrs, AttributeSet(),
                 {})
 __OMP_RTL_ATTRS(omp_get_max_active_levels, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_level, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_ancestor_thread_num, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_team_size, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_active_level, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_in_final, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_proc_bind, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_num_places, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_num_procs, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_place_proc_ids, GetterArgWriteAttrs, AttributeSet(),
                 ArrayRef<AttributeSet>({AttributeSet(),
                                         AttributeSet(EnumAttr(NoCapture),
                                                      EnumAttr(WriteOnly))}))
 __OMP_RTL_ATTRS(omp_get_place_num, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_partition_num_places, GetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_get_partition_place_nums, GetterAttrs, AttributeSet(), {})
 
 __OMP_RTL_ATTRS(omp_set_num_threads, SetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_set_dynamic, SetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_set_nested, SetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_set_schedule, SetterAttrs, AttributeSet(), {})
 __OMP_RTL_ATTRS(omp_set_max_active_levels, SetterAttrs, AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_master,
                 AttributeSet(EnumAttr(InaccessibleMemOrArgMemOnly)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_end_master,
                 AttributeSet(EnumAttr(InaccessibleMemOrArgMemOnly)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_critical,
                 AttributeSet(EnumAttr(InaccessibleMemOrArgMemOnly)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_critical_with_hint,
                 AttributeSet(EnumAttr(InaccessibleMemOrArgMemOnly)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_end_critical,
                 AttributeSet(EnumAttr(InaccessibleMemOrArgMemOnly)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_begin, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_end, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_reduce, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_reduce_nowait, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_end_reduce, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_end_reduce_nowait, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_ordered, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_end_ordered, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_for_static_init_4, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_for_static_init_4u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_for_static_init_8, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_for_static_init_8u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_for_static_fini, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dist_dispatch_init_4, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dist_dispatch_init_4u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dist_dispatch_init_8, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dist_dispatch_init_8u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_init_4, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_init_4u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_init_8, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_init_8u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_next_4, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_next_4u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_next_8, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_next_8u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_fini_4, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_fini_4u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_fini_8, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dispatch_fini_8u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_team_static_init_4, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_team_static_init_4u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_team_static_init_8, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_team_static_init_8u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dist_for_static_init_4, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dist_for_static_init_4u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dist_for_static_init_8, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_dist_for_static_init_8u, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_single, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_end_single, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_omp_task_alloc, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_omp_task, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_end_taskgroup, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_taskgroup, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_omp_task_begin_if0, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_omp_task_complete_if0, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_omp_task_with_deps, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_taskloop, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_omp_target_task_alloc,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_taskred_modifier_init,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_taskred_init,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_task_reduction_modifier_fini,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_task_reduction_get_th_data,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_task_reduction_init,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_task_reduction_modifier_init,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_proxy_task_completed_ooo,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_omp_wait_deps, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_cancellationpoint, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_fork_teams, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_push_num_teams, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_copyprivate, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_threadprivate_cached, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_threadprivate_register, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_doacross_init, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_doacross_post, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_doacross_wait, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_doacross_fini, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_alloc, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_free, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_init_allocator, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_destroy_allocator, AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 __OMP_RTL_ATTRS(__kmpc_push_target_tripcount,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_target,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_target_nowait,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_target_teams,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_target_teams_nowait,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_register_requires,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_target_data_begin,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_target_data_begin_nowait,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_target_data_end,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_target_data_end_nowait,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_target_data_update,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_target_data_update_nowait,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_mapper_num_components,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__tgt_push_mapper_component,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 __OMP_RTL_ATTRS(__kmpc_task_allow_completion_event,
                 AttributeSet(EnumAttr(NoUnwind)),
                 AttributeSet(), {})
 
 #undef __OMP_RTL_ATTRS
 #undef OMP_RTL_ATTRS
 #undef AttributeSet
 #undef EnumAttr
 
 ///}
 
 /// KMP ident_t bit flags
 ///
 /// In accordance with the values in `openmp/runtime/src/kmp.h`.
 ///
 ///{
 
 #ifndef OMP_IDENT_FLAG
 #define OMP_IDENT_FLAG(Enum, Str, Value)
 #endif
 
 #define __OMP_IDENT_FLAG(Name, Value)                                          \
   OMP_IDENT_FLAG(OMP_IDENT_FLAG_##Name, #Name, Value)
 
 __OMP_IDENT_FLAG(KMPC, 0x02)
 __OMP_IDENT_FLAG(BARRIER_EXPL, 0x20)
 __OMP_IDENT_FLAG(BARRIER_IMPL, 0x0040)
 __OMP_IDENT_FLAG(BARRIER_IMPL_MASK, 0x01C0)
 __OMP_IDENT_FLAG(BARRIER_IMPL_FOR, 0x0040)
 __OMP_IDENT_FLAG(BARRIER_IMPL_SECTIONS, 0x00C0)
 __OMP_IDENT_FLAG(BARRIER_IMPL_SINGLE, 0x0140)
 __OMP_IDENT_FLAG(BARRIER_IMPL_WORKSHARE, 0x01C0)
 
 #undef __OMP_IDENT_FLAG
 #undef OMP_IDENT_FLAG
 
 ///}
 
 /// KMP cancel kind
 ///
 ///{
 
 #ifndef OMP_CANCEL_KIND
 #define OMP_CANCEL_KIND(Enum, Str, DirectiveEnum, Value)
 #endif
 
 #define __OMP_CANCEL_KIND(Name, Value)                                         \
   OMP_CANCEL_KIND(OMP_CANCEL_KIND_##Name, #Name, OMPD_##Name, Value)
 
 __OMP_CANCEL_KIND(parallel, 1)
 __OMP_CANCEL_KIND(for, 2)
 __OMP_CANCEL_KIND(sections, 3)
 __OMP_CANCEL_KIND(taskgroup, 4)
 
 #undef __OMP_CANCEL_KIND
 #undef OMP_CANCEL_KIND
 
 ///}
 
 /// Default kinds
 ///
 ///{
 
 #ifndef OMP_DEFAULT_KIND
 #define OMP_DEFAULT_KIND(Enum, Str)
 #endif
 
 #define __OMP_DEFAULT_KIND(Name) OMP_DEFAULT_KIND(OMP_DEFAULT_##Name, #Name)
 
 __OMP_DEFAULT_KIND(none)
 __OMP_DEFAULT_KIND(shared)
 __OMP_DEFAULT_KIND(unknown)
 
 #undef __OMP_DEFAULT_KIND
 #undef OMP_DEFAULT_KIND
 
 ///}
 
 /// Proc bind kinds
 ///
 ///{
 
 #ifndef OMP_PROC_BIND_KIND
 #define OMP_PROC_BIND_KIND(Enum, Str, Value)
 #endif
 
 #define __OMP_PROC_BIND_KIND(Name, Value)                                      \
   OMP_PROC_BIND_KIND(OMP_PROC_BIND_##Name, #Name, Value)
 
 __OMP_PROC_BIND_KIND(master, 2)
 __OMP_PROC_BIND_KIND(close, 3)
 __OMP_PROC_BIND_KIND(spread, 4)
 __OMP_PROC_BIND_KIND(default, 6)
 __OMP_PROC_BIND_KIND(unknown, 7)
 
 #undef __OMP_PROC_BIND_KIND
 #undef OMP_PROC_BIND_KIND
 
 ///}
 
 /// OpenMP context related definitions:
 ///  - trait set selector
 ///  - trait selector
 ///  - trait property
 ///
 ///{
 
 #ifndef OMP_TRAIT_SET
 #define OMP_TRAIT_SET(Enum, Str)
 #endif
 #ifndef OMP_TRAIT_SELECTOR
 #define OMP_TRAIT_SELECTOR(Enum, TraitSetEnum, Str, RequiresProperty)
 #endif
 #ifndef OMP_TRAIT_PROPERTY
 #define OMP_TRAIT_PROPERTY(Enum, TraitSetEnum, TraitSelectorEnum, Str)
 #endif
 #ifndef OMP_LAST_TRAIT_PROPERTY
 #define OMP_LAST_TRAIT_PROPERTY(Enum)
 #endif
 
 #define __OMP_TRAIT_SET(Name) OMP_TRAIT_SET(Name, #Name)
 #define __OMP_TRAIT_SELECTOR(TraitSet, Name, RequiresProperty)                 \
   OMP_TRAIT_SELECTOR(TraitSet##_##Name, TraitSet, #Name, RequiresProperty)
 #define __OMP_TRAIT_SELECTOR_AND_PROPERTY(TraitSet, Name)                      \
   OMP_TRAIT_SELECTOR(TraitSet##_##Name, TraitSet, #Name, false)                \
   OMP_TRAIT_PROPERTY(TraitSet##_##Name##_##Name, TraitSet, TraitSet##_##Name,  \
                      #Name)
 #define __OMP_TRAIT_PROPERTY(TraitSet, TraitSelector, Name)                    \
   OMP_TRAIT_PROPERTY(TraitSet##_##TraitSelector##_##Name, TraitSet,            \
                      TraitSet##_##TraitSelector, #Name)
 
 // "invalid" must go first.
 OMP_TRAIT_SET(invalid, "invalid")
 OMP_TRAIT_SELECTOR(invalid, invalid, "invalid", false)
 OMP_TRAIT_PROPERTY(invalid, invalid, invalid, "invalid")
 
 __OMP_TRAIT_SET(construct)
 __OMP_TRAIT_SELECTOR_AND_PROPERTY(construct, target)
 __OMP_TRAIT_SELECTOR_AND_PROPERTY(construct, teams)
 __OMP_TRAIT_SELECTOR_AND_PROPERTY(construct, parallel)
 __OMP_TRAIT_SELECTOR_AND_PROPERTY(construct, for)
 __OMP_TRAIT_SELECTOR_AND_PROPERTY(construct, simd)
 
 __OMP_TRAIT_SET(device)
 
 __OMP_TRAIT_SELECTOR(device, kind, true)
 
 __OMP_TRAIT_PROPERTY(device, kind, host)
 __OMP_TRAIT_PROPERTY(device, kind, nohost)
 __OMP_TRAIT_PROPERTY(device, kind, cpu)
 __OMP_TRAIT_PROPERTY(device, kind, gpu)
 __OMP_TRAIT_PROPERTY(device, kind, fpga)
 __OMP_TRAIT_PROPERTY(device, kind, any)
 
 __OMP_TRAIT_SELECTOR(device, isa, true)
 
 // TODO: What do we want for ISA?
 
 __OMP_TRAIT_SELECTOR(device, arch, true)
 
 __OMP_TRAIT_PROPERTY(device, arch, arm)
 __OMP_TRAIT_PROPERTY(device, arch, armeb)
 __OMP_TRAIT_PROPERTY(device, arch, aarch64)
 __OMP_TRAIT_PROPERTY(device, arch, aarch64_be)
 __OMP_TRAIT_PROPERTY(device, arch, aarch64_32)
 __OMP_TRAIT_PROPERTY(device, arch, ppc)
 __OMP_TRAIT_PROPERTY(device, arch, ppc64)
 __OMP_TRAIT_PROPERTY(device, arch, ppc64le)
 __OMP_TRAIT_PROPERTY(device, arch, x86)
 __OMP_TRAIT_PROPERTY(device, arch, x86_64)
 __OMP_TRAIT_PROPERTY(device, arch, amdgcn)
 __OMP_TRAIT_PROPERTY(device, arch, nvptx)
 __OMP_TRAIT_PROPERTY(device, arch, nvptx64)
 
 __OMP_TRAIT_SET(implementation)
 
 __OMP_TRAIT_SELECTOR(implementation, vendor, true)
 
 __OMP_TRAIT_PROPERTY(implementation, vendor, amd)
 __OMP_TRAIT_PROPERTY(implementation, vendor, arm)
 __OMP_TRAIT_PROPERTY(implementation, vendor, bsc)
 __OMP_TRAIT_PROPERTY(implementation, vendor, cray)
 __OMP_TRAIT_PROPERTY(implementation, vendor, fujitsu)
 __OMP_TRAIT_PROPERTY(implementation, vendor, gnu)
 __OMP_TRAIT_PROPERTY(implementation, vendor, ibm)
 __OMP_TRAIT_PROPERTY(implementation, vendor, intel)
 __OMP_TRAIT_PROPERTY(implementation, vendor, llvm)
 __OMP_TRAIT_PROPERTY(implementation, vendor, pgi)
 __OMP_TRAIT_PROPERTY(implementation, vendor, ti)
 __OMP_TRAIT_PROPERTY(implementation, vendor, unknown)
 
 __OMP_TRAIT_SELECTOR(implementation, extension, true)
 __OMP_TRAIT_PROPERTY(implementation, extension, match_all)
 __OMP_TRAIT_PROPERTY(implementation, extension, match_any)
 __OMP_TRAIT_PROPERTY(implementation, extension, match_none)
 
 __OMP_TRAIT_SET(user)
 
 __OMP_TRAIT_SELECTOR(user, condition, true)
 
 __OMP_TRAIT_PROPERTY(user, condition, true)
 __OMP_TRAIT_PROPERTY(user, condition, false)
 __OMP_TRAIT_PROPERTY(user, condition, unknown)
 
 #undef OMP_TRAIT_SET
 #undef __OMP_TRAIT_SET
 ///}
 
 /// Traits for the requires directive
 ///
 /// These will (potentially) become trait selectors for the OpenMP context if
 /// the OMP_REQUIRES_TRAIT macro is not defined.
 ///
 ///{
 
 #ifdef OMP_REQUIRES_TRAIT
 #define __OMP_REQUIRES_TRAIT(Name)                                             \
   OMP_REQUIRES_TRAIT(OMP_REQUIRES_TRAIT_##Name, #Name)
 #else
 #define __OMP_REQUIRES_TRAIT(Name)                                             \
   __OMP_TRAIT_SELECTOR_AND_PROPERTY(implementation, Name)
 #endif
 
 __OMP_REQUIRES_TRAIT(unified_address)
 __OMP_REQUIRES_TRAIT(unified_shared_memory)
 __OMP_REQUIRES_TRAIT(reverse_offload)
 __OMP_REQUIRES_TRAIT(dynamic_allocators)
 __OMP_REQUIRES_TRAIT(atomic_default_mem_order)
 
 OMP_LAST_TRAIT_PROPERTY(
     implementation_atomic_default_mem_order_atomic_default_mem_order)
 
 #undef __OMP_TRAIT_SELECTOR_AND_PROPERTY
 #undef OMP_TRAIT_SELECTOR
 #undef __OMP_TRAIT_SELECTOR
 #undef OMP_TRAIT_PROPERTY
 #undef OMP_LAST_TRAIT_PROPERTY
 #undef __OMP_TRAIT_PROPERTY
 #undef __OMP_REQUIRES_TRAIT
 #undef OMP_REQUIRES_TRAIT
 ///}
diff --git a/llvm/include/llvm/Transforms/IPO/OpenMPOpt.h b/llvm/include/llvm/Transforms/IPO/OpenMPOpt.h
index e9702ef7e19..15a7af0f0ea 100644
--- a/llvm/include/llvm/Transforms/IPO/OpenMPOpt.h
+++ b/llvm/include/llvm/Transforms/IPO/OpenMPOpt.h
@@ -1,412 +1,424 @@
 //===- IPO/OpenMPOpt.h - Collection of OpenMP optimizations -----*- C++ -*-===//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
 // See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
 //===----------------------------------------------------------------------===//
 
 #ifndef LLVM_TRANSFORMS_IPO_OPENMP_OPT_H
 #define LLVM_TRANSFORMS_IPO_OPENMP_OPT_H
 
 #include "llvm/ADT/EnumeratedArray.h"
 #include "llvm/Analysis/OptimizationRemarkEmitter.h"
 #include "llvm/Frontend/OpenMP/OMPConstants.h"
 #include "llvm/Frontend/OpenMP/OMPIRBuilder.h"
 #include "llvm/Transforms/IPO/Attributor.h"
 #include "llvm/Transforms/Utils/CallGraphUpdater.h"
 #include "llvm/Analysis/CGSCCPassManager.h"
 #include "llvm/Analysis/LazyCallGraph.h"
 #include "llvm/IR/PassManager.h"
 #include "llvm/Analysis/MemorySSA.h"
 
 namespace llvm {
 namespace omp {
 
 using namespace types;
 
 /// OpenMP specific information. For now, stores RFIs and ICVs also needed for
 /// Attributor runs.
 struct OMPInformationCache : public InformationCache {
   OMPInformationCache(Module &M, AnalysisGetter &AG,
                       BumpPtrAllocator &Allocator, SetVector<Function *> *CGSCC,
                       SmallPtrSetImpl<Function *> &ModuleSlice)
       : InformationCache(M, AG, Allocator, CGSCC), ModuleSlice(ModuleSlice),
         OMPBuilder(M) {
     OMPBuilder.initialize();
     initializeRuntimeFunctions();
     initializeInternalControlVars();
   }
 
   /// Generic information that describes an internal control variable.
   struct InternalControlVarInfo {
     /// The kind, as described by InternalControlVar enum.
     InternalControlVar Kind;
 
     /// The name of the ICV.
     StringRef Name;
 
     /// Environment variable associated with this ICV.
     StringRef EnvVarName;
 
     /// Initial value kind.
     ICVInitValue InitKind;
 
     /// Initial value.
     ConstantInt *InitValue;
 
     /// Setter RTL function associated with this ICV.
     RuntimeFunction Setter;
 
     /// Getter RTL function associated with this ICV.
     RuntimeFunction Getter;
 
     /// RTL Function corresponding to the override clause of this ICV
     RuntimeFunction Clause;
   };
 
   /// Generic information that describes a runtime function
   struct RuntimeFunctionInfo {
 
     /// The kind, as described by the RuntimeFunction enum.
     RuntimeFunction Kind;
 
     /// The name of the function.
     StringRef Name;
 
     /// Flag to indicate a variadic function.
     bool IsVarArg;
 
     /// The return type of the function.
     Type *ReturnType;
 
     /// The argument types of the function.
     SmallVector<Type *, 8> ArgumentTypes;
 
     /// The declaration if available.
     Function *Declaration = nullptr;
 
     /// Uses of this runtime function per function containing the use.
     using UseVector = SmallVector<Use *, 16>;
 
     /// Return the vector of uses in function \p F.
     UseVector &getOrCreateUseVector(Function *F) {
       std::shared_ptr<UseVector> &UV = UsesMap[F];
       if (!UV)
         UV = std::make_shared<UseVector>();
       return *UV;
     }
 
     /// Return the vector of uses in function \p F or `nullptr` if there are
     /// none.
     const UseVector *getUseVector(Function &F) const {
       auto I = UsesMap.find(&F);
       if (I != UsesMap.end())
         return I->second.get();
       return nullptr;
     }
 
     /// Return how many functions contain uses of this runtime function.
     size_t getNumFunctionsWithUses() const { return UsesMap.size(); }
 
     /// Return the number of arguments (or the minimal number for variadic
     /// functions).
     size_t getNumArgs() const { return ArgumentTypes.size(); }
 
     /// Run the callback \p CB on each use and forget the use if the result is
     /// true. The callback will be fed the function in which the use was
     /// encountered as second argument.
     void foreachUse(function_ref<bool(Use &, Function &)> CB) {
       for (auto &It : UsesMap)
         foreachUse(CB, It.first, It.second.get());
     }
 
     /// Run the callback \p CB on each use within the function \p F and forget
     /// the use if the result is true.
     void foreachUse(function_ref<bool(Use &, Function &)> CB, Function *F,
                     UseVector *Uses = nullptr);
 
   private:
     /// Map from functions to all uses of this runtime function contained in
     /// them.
     DenseMap<Function *, std::shared_ptr<UseVector>> UsesMap;
   };
 
   /// Used to store information about a runtime call that involves
   /// host to device memory offloading. For example:
   /// __tgt_target_data_begin(...,
   ///   i8** %offload_baseptrs, i8** %offload_ptrs, i64* %offload_sizes,
   /// ...)
   struct MemoryTransfer {
 
     /// Used to map the values physically (in the IR) stored in an offload
     /// array, to a vector in memory.
     struct OffloadArray {
       AllocaInst &Array; /// Physical array (in the IR).
       SmallVector<Value *, 8> StoredValues; /// Mapped values.
       SmallVector<StoreInst *, 8> LastAccesses;
       InformationCache &InfoCache;
 
       /// Factory function for creating and initializing the OffloadArray with
       /// the values stored in \p Array before the instruction \p Before is
       /// reached.
       /// This MUST be used instead of the constructor.
       static std::unique_ptr<OffloadArray> initialize(
           AllocaInst &Array,
           Instruction &Before,
           InformationCache &InfoCache);
 
       /// Use the factory function initialize(...) instead.
       OffloadArray(AllocaInst &Array, InformationCache &InfoCache)
           : Array(Array), InfoCache(InfoCache) {}
 
     private:
       /// Traverses the BasicBlocks collecting the stores made to
       /// Array, leaving StoredValues with the values stored before
       /// the instruction \p Before is reached.
       bool getValues(Instruction &Before);
 
       /// Returns the index of Array where the store is being
       /// made. Returns -1 if the index can't be deduced.
       int32_t getAccessedIdx(StoreInst &S);
 
       /// Returns true if all values in StoredValues and
       /// LastAccesses are not nullptrs.
       bool isFilled();
     };
 
     CallInst *RuntimeCall; /// Call that involves a memotry transfer.
     OMPInformationCache &InfoCache;
 
     /// These help mapping the values in offload_baseptrs, offload_ptrs, and
     /// offload_sizes, respectively.
+    const unsigned BasePtrsArgNum = 2;
     std::unique_ptr<OffloadArray> BasePtrs = nullptr;
+    const unsigned PtrsArgNum = 3;
     std::unique_ptr<OffloadArray> Ptrs = nullptr;
+    const unsigned SizesArgNum = 4;
     std::unique_ptr<OffloadArray> Sizes = nullptr;
 
     /// Set of instructions that compose the argument setup for the call
     /// RuntimeCall.
     SetVector<Instruction *> Issue;
 
     MemoryTransfer(CallInst *RuntimeCall, OMPInformationCache &InfoCache) :
         RuntimeCall{RuntimeCall}, InfoCache{InfoCache}
     {}
 
     /// Maps the values physically (the IR) stored in the offload arrays
     /// offload_baseptrs, offload_ptrs, offload_sizes to their corresponding
     /// members, BasePtrs, Ptrs, Sizes.
     /// Returns false if one of the arrays couldn't be processed or some of the
     /// values couldn't be found.
     bool getValuesInOffloadArrays();
 
     /// Groups the instructions that compose the argument setup for the call
     /// RuntimeCall.
     bool detectIssue();
 
     /// Returns true if \p I might modify some of the values in the
     /// offload arrays.
     bool mayBeModifiedBy(Instruction *I);
 
+    /// Splits this object into its "issue" and "wait" corresponding runtime
+    /// calls. The "issue" is moved after \p After and the "wait" is moved
+    /// before \p Before.
+    bool split(Instruction *After, Instruction *Before);
+
   private:
     /// Gets the setup instructions for each of the values in \p OA. These
     /// instructions are stored into Issue.
     bool getSetupInstructions(std::unique_ptr<OffloadArray> &OA);
     /// Gets the setup instructions for the pointer operand of \p S.
     bool getPointerSetupInstructions(StoreInst *S);
     /// Gets the setup instructions for the value operand of \p S.
     bool getValueSetupInstructions(StoreInst *S);
 
     /// Returns true if \p I may modify one of the values in \p Values.
     bool mayModify(Instruction *I, SmallVectorImpl<Value *> &Values);
+
+    /// Removes from the function all the instructions in Issue and inserts
+    /// them after \p After.
+    void moveIssue(Instruction *After);
   };
 
   /// The slice of the module we are allowed to look at.
   SmallPtrSetImpl<Function *> &ModuleSlice;
 
   /// An OpenMP-IR-Builder instance
   OpenMPIRBuilder OMPBuilder;
 
   /// Map from runtime function kind to the runtime function description.
   EnumeratedArray<RuntimeFunctionInfo, RuntimeFunction,
       RuntimeFunction::OMPRTL___last>
       RFIs;
 
   /// Map from ICV kind to the ICV description.
   EnumeratedArray<InternalControlVarInfo, InternalControlVar,
       InternalControlVar::ICV___last>
       ICVs;
 
   /// Helper to initialize all internal control variable information for those
   /// defined in OMPKinds.def.
   void initializeInternalControlVars();
 
   /// Helper to initialize all runtime function information for those defined
   /// in OpenMPKinds.def.
   void initializeRuntimeFunctions();
 
   /// Returns true if the function declaration \p F matches the runtime
   /// function types, that is, return type \p RTFRetType, and argument types
   /// \p RTFArgTypes.
   static bool declMatchesRTFTypes(Function *F, Type *RTFRetType,
                                   SmallVector<Type *, 8> &RTFArgTypes);
 };
 
 struct OpenMPOpt {
 
   using MemoryTransfer = OMPInformationCache::MemoryTransfer;
   using OptimizationRemarkGetter =
   function_ref<OptimizationRemarkEmitter &(Function *)>;
 
   OpenMPOpt(SmallVectorImpl<Function *> &SCC, CallGraphUpdater &CGUpdater,
             OptimizationRemarkGetter OREGetter,
             OMPInformationCache &OMPInfoCache, Attributor &A)
       : M(*(*SCC.begin())->getParent()), SCC(SCC), CGUpdater(CGUpdater),
         OREGetter(OREGetter), OMPInfoCache(OMPInfoCache), A(A) {}
 
   /// Run all OpenMP optimizations on the underlying SCC/ModuleSlice.
   bool run();
 
   /// Return the call if \p U is a callee use in a regular call. If \p RFI is
   /// given it has to be the callee or a nullptr is returned.
   static CallInst *getCallIfRegularCall(
       Use &U, OMPInformationCache::RuntimeFunctionInfo *RFI = nullptr);
 
   /// Return the call if \p V is a regular call. If \p RFI is given it has to be
   /// the callee or a nullptr is returned.
   static CallInst *getCallIfRegularCall(
       Value &V, OMPInformationCache::RuntimeFunctionInfo *RFI = nullptr);
 
   /// Returns the integer representation of \p V.
   static int64_t getIntLiteral(const Value *V) {
     assert(V && "Getting Integer value of nullptr");
     return (dyn_cast<ConstantInt>(V))->getZExtValue();
   }
 
 private:
   /// Try to delete parallel regions if possible.
   bool deleteParallelRegions();
 
   /// Try to eliminiate runtime calls by reusing existing ones.
   bool deduplicateRuntimeCalls();
 
   /// Tries to hide the latency of runtime calls that involve host to
   /// device memory transfers by splitting them into their "issue" and "wait".
   /// versions. The "issue" is moved upwards as much as possible. The "wait" is
   /// moved downards as much as possible. The "issue" issues the memory transfer
   /// asynchronously, returning a handle. The "wait" waits in the returned
   /// handle for the memory transfer to finish.
   bool hideMemTransfersLatency();
 
   /// Returns a pointer to the instruction where the "issue" of \p MT can be
   /// moved. Returns nullptr if the movement is not possible, or not worth it.
   Instruction *canBeMovedUpwards(MemoryTransfer &MT);
 
   /// Returns a pointer to the instruction where the "wait" of \p MT can be
   /// moved. Returns nullptr if the movement is not possible, or not worth it.
   Instruction *canBeMovedDownwards(MemoryTransfer &MT);
 
   static Value *combinedIdentStruct(Value *CurrentIdent, Value *NextIdent,
                                     bool GlobalOnly, bool &SingleChoice);
 
   /// Return an `struct ident_t*` value that represents the ones used in the
   /// calls of \p RFI inside of \p F. If \p GlobalOnly is true, we will not
   /// return a local `struct ident_t*`. For now, if we cannot find a suitable
   /// return value we create one from scratch. We also do not yet combine
   /// information, e.g., the source locations, see combinedIdentStruct.
   Value *
   getCombinedIdentFromCallUsesIn(OMPInformationCache::RuntimeFunctionInfo &RFI,
                                  Function &F, bool GlobalOnly);
 
   /// Try to eliminiate calls of \p RFI in \p F by reusing an existing one or
   /// \p ReplVal if given.
   bool deduplicateRuntimeCalls(Function &F,
                                OMPInformationCache::RuntimeFunctionInfo &RFI,
                                Value *ReplVal = nullptr);
 
   /// Collect arguments that represent the global thread id in \p GTIdArgs.
   void collectGlobalThreadIdArguments(SmallSetVector<Value *, 16> &GTIdArgs);
 
   /// Emit a remark generically
   ///
   /// This template function can be used to generically emit a remark. The
   /// RemarkKind should be one of the following:
   ///   - OptimizationRemark to indicate a successful optimization attempt
   ///   - OptimizationRemarkMissed to report a failed optimization attempt
   ///   - OptimizationRemarkAnalysis to provide additional information about an
   ///     optimization attempt
   ///
   /// The remark is built using a callback function provided by the caller that
   /// takes a RemarkKind as input and returns a RemarkKind.
   template <typename RemarkKind,
       typename RemarkCallBack = function_ref<RemarkKind(RemarkKind &&)>>
   void emitRemark(Instruction *Inst, StringRef RemarkName,
                   RemarkCallBack &&RemarkCB);
 
   /// Emit a remark on a function. Since only OptimizationRemark is supporting
   /// this, it can't be made generic.
   void emitRemarkOnFunction(
       Function *F, StringRef RemarkName,
       function_ref<OptimizationRemark(OptimizationRemark &&)> &&RemarkCB);
 
   /// The underyling module.
   Module &M;
 
   /// The SCC we are operating on.
   SmallVectorImpl<Function *> &SCC;
 
   /// Callback to update the call graph, the first argument is a removed call,
   /// the second an optional replacement call.
   CallGraphUpdater &CGUpdater;
 
   /// Callback to get an OptimizationRemarkEmitter from a Function *
   OptimizationRemarkGetter OREGetter;
 
   /// OpenMP-specific information cache. Also Used for Attributor runs.
   OMPInformationCache &OMPInfoCache;
 
   /// Attributor instance.
   Attributor &A;
 
   /// Helper function to run Attributor on SCC.
   bool runAttributor();
 
   /// Populate the Attributor with abstract attribute opportunities in the
   /// function.
   void registerAAs();
 };
 
 /// Helper to remember if the module contains OpenMP (runtime calls), to be used
 /// foremost with containsOpenMP.
 struct OpenMPInModule {
   OpenMPInModule &operator=(bool Found) {
     if (Found)
       Value = OpenMPInModule::OpenMP::FOUND;
     else
       Value = OpenMPInModule::OpenMP::NOT_FOUND;
     return *this;
   }
   bool isKnown() { return Value != OpenMP::UNKNOWN; }
   operator bool() { return Value != OpenMP::NOT_FOUND; }
 
 private:
   enum class OpenMP { FOUND, NOT_FOUND, UNKNOWN } Value = OpenMP::UNKNOWN;
 };
 
 /// Helper to determine if \p M contains OpenMP (runtime calls).
 bool containsOpenMP(Module &M, OpenMPInModule &OMPInModule);
 
 } // namespace omp
 
 /// OpenMP optimizations pass.
 class OpenMPOptPass : public PassInfoMixin<OpenMPOptPass> {
   /// Helper to remember if the module contains OpenMP (runtime calls).
   omp::OpenMPInModule OMPInModule;
 
 public:
   PreservedAnalyses run(LazyCallGraph::SCC &C, CGSCCAnalysisManager &AM,
                         LazyCallGraph &CG, CGSCCUpdateResult &UR);
 };
 
 } // end namespace llvm
 
 #endif // LLVM_TRANSFORMS_IPO_OPENMP_OPT_H
diff --git a/llvm/lib/Transforms/IPO/OpenMPOpt.cpp b/llvm/lib/Transforms/IPO/OpenMPOpt.cpp
index cce3718ea38..11b142512e9 100644
--- a/llvm/lib/Transforms/IPO/OpenMPOpt.cpp
+++ b/llvm/lib/Transforms/IPO/OpenMPOpt.cpp
@@ -1,1387 +1,1459 @@
 //===-- IPO/OpenMPOpt.cpp - Collection of OpenMP specific optimizations ---===//
 //
 // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
 // See https://llvm.org/LICENSE.txt for license information.
 // SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
 //
 //===----------------------------------------------------------------------===//
 //
 // OpenMP specific optimizations:
 //
 // - Deduplication of runtime calls, e.g., omp_get_thread_num.
 //
 //===----------------------------------------------------------------------===//
 
 #include "llvm/Transforms/IPO/OpenMPOpt.h"
 
 #include "llvm/ADT/EnumeratedArray.h"
 #include "llvm/ADT/Statistic.h"
 #include "llvm/Analysis/CallGraph.h"
 #include "llvm/Analysis/CallGraphSCCPass.h"
 #include "llvm/Analysis/OptimizationRemarkEmitter.h"
 #include "llvm/Frontend/OpenMP/OMPConstants.h"
 #include "llvm/Frontend/OpenMP/OMPIRBuilder.h"
 #include "llvm/InitializePasses.h"
 #include "llvm/Support/CommandLine.h"
 #include "llvm/Transforms/IPO.h"
 #include "llvm/Transforms/IPO/Attributor.h"
 #include "llvm/Transforms/Utils/CallGraphUpdater.h"
 #include "llvm/Analysis/ValueTracking.h"
 #include "llvm/Analysis/MemorySSA.h"
 #include "llvm/Analysis/AliasAnalysis.h"
 
 using namespace llvm;
 using namespace omp;
 using namespace types;
 
 #define DEBUG_TYPE "openmp-opt"
 
 static cl::opt<bool> DisableOpenMPOptimizations(
     "openmp-opt-disable", cl::ZeroOrMore,
     cl::desc("Disable OpenMP specific optimizations."), cl::Hidden,
     cl::init(false));
 
 static cl::opt<bool> PrintICVValues("openmp-print-icv-values", cl::init(false),
                                     cl::Hidden);
 
+static cl::opt<bool> SplitMemoryTransfers(
+    "openmp-split-memtransfers",
+    cl::desc("Tries to hide the latency of host to device memory transfers"),
+    cl::Hidden, cl::init(false));
+
 STATISTIC(NumOpenMPRuntimeCallsDeduplicated,
           "Number of OpenMP runtime calls deduplicated");
 STATISTIC(NumOpenMPParallelRegionsDeleted,
           "Number of OpenMP parallel regions deleted");
 STATISTIC(NumOpenMPRuntimeFunctionsIdentified,
           "Number of OpenMP runtime functions identified");
 STATISTIC(NumOpenMPRuntimeFunctionUsesIdentified,
           "Number of OpenMP runtime function uses identified");
 
 #if !defined(NDEBUG)
 static constexpr auto TAG = "[" DEBUG_TYPE "]";
 #endif
 
 /// Helper struct to store tracked ICV values at specif instructions.
 struct ICVValue {
   Instruction *Inst;
   Value *TrackedValue;
 
   ICVValue(Instruction *I, Value *Val) : Inst(I), TrackedValue(Val) {}
 };
 
 namespace llvm {
 
 // Provide DenseMapInfo for ICVValue
 template <> struct DenseMapInfo<ICVValue> {
   using InstInfo = DenseMapInfo<Instruction *>;
   using ValueInfo = DenseMapInfo<Value *>;
 
   static inline ICVValue getEmptyKey() {
     return ICVValue(InstInfo::getEmptyKey(), ValueInfo::getEmptyKey());
   };
 
   static inline ICVValue getTombstoneKey() {
     return ICVValue(InstInfo::getTombstoneKey(), ValueInfo::getTombstoneKey());
   };
 
   static unsigned getHashValue(const ICVValue &ICVVal) {
     return detail::combineHashValue(
         InstInfo::getHashValue(ICVVal.Inst),
         ValueInfo::getHashValue(ICVVal.TrackedValue));
   }
 
   static bool isEqual(const ICVValue &LHS, const ICVValue &RHS) {
     return InstInfo::isEqual(LHS.Inst, RHS.Inst) &&
            ValueInfo::isEqual(LHS.TrackedValue, RHS.TrackedValue);
   }
 };
 
 } // end namespace llvm
 
 //===----------------------------------------------------------------------===//
 // Definitions of the OMPInformationCache helper structure.
 //===----------------------------------------------------------------------===//
 
 using MemoryTransfer = OMPInformationCache::MemoryTransfer;
 using OffloadArray = MemoryTransfer::OffloadArray;
 
 void OMPInformationCache::RuntimeFunctionInfo::foreachUse(
     function_ref<bool(Use &, Function &)> CB, Function *F, UseVector *Uses) {
   SmallVector<unsigned, 8> ToBeDeleted;
   ToBeDeleted.clear();
 
   unsigned Idx = 0;
   UseVector &UV = Uses ? *Uses : getOrCreateUseVector(F);
 
   for (Use *U : UV) {
     if (CB(*U, *F))
       ToBeDeleted.push_back(Idx);
     ++Idx;
   }
 
   // Remove the to-be-deleted indices in reverse order as prior
   // modifcations will not modify the smaller indices.
   while (!ToBeDeleted.empty()) {
     unsigned Idx = ToBeDeleted.pop_back_val();
     UV[Idx] = UV.back();
     UV.pop_back();
   }
 }
 
 void OMPInformationCache::initializeInternalControlVars() {
 #define ICV_RT_SET(_Name, RTL)                                                 \
   {                                                                            \
     auto &ICV = ICVs[_Name];                                                   \
     ICV.Setter = RTL;                                                          \
   }
 #define ICV_RT_GET(Name, RTL)                                                  \
   {                                                                            \
     auto &ICV = ICVs[Name];                                                    \
     ICV.Getter = RTL;                                                          \
   }
 #define ICV_DATA_ENV(Enum, _Name, _EnvVarName, Init)                           \
   {                                                                            \
     auto &ICV = ICVs[Enum];                                                    \
     ICV.Name = _Name;                                                          \
     ICV.Kind = Enum;                                                           \
     ICV.InitKind = Init;                                                       \
     ICV.EnvVarName = _EnvVarName;                                              \
     switch (ICV.InitKind) {                                                    \
     case ICV_IMPLEMENTATION_DEFINED:                                           \
       ICV.InitValue = nullptr;                                                 \
       break;                                                                   \
     case ICV_ZERO:                                                             \
       ICV.InitValue =                                                          \
           ConstantInt::get(Type::getInt32Ty(Int32->getContext()), 0);          \
       break;                                                                   \
     case ICV_FALSE:                                                            \
       ICV.InitValue = ConstantInt::getFalse(Int1->getContext());               \
       break;                                                                   \
     case ICV_LAST:                                                             \
       break;                                                                   \
     }                                                                          \
   }
 #include "llvm/Frontend/OpenMP/OMPKinds.def"
 }
 
 void OMPInformationCache::initializeRuntimeFunctions() {
   // Helper to collect all uses of the decleration in the UsesMap.
   auto CollectUses = [&](RuntimeFunctionInfo &RFI) {
     unsigned NumUses = 0;
     if (!RFI.Declaration)
       return NumUses;
     OMPBuilder.addAttributes(RFI.Kind, *RFI.Declaration);
 
     NumOpenMPRuntimeFunctionsIdentified += 1;
     NumOpenMPRuntimeFunctionUsesIdentified += RFI.Declaration->getNumUses();
 
     // TODO: We directly convert uses into proper calls and unknown uses.
     for (Use &U : RFI.Declaration->uses()) {
       if (Instruction *UserI = dyn_cast<Instruction>(U.getUser())) {
         if (ModuleSlice.count(UserI->getFunction())) {
           RFI.getOrCreateUseVector(UserI->getFunction()).push_back(&U);
           ++NumUses;
         }
       } else {
         RFI.getOrCreateUseVector(nullptr).push_back(&U);
         ++NumUses;
       }
     }
     return NumUses;
   };
 
   Module &M = *((*ModuleSlice.begin())->getParent());
 
 #define OMP_RTL(_Enum, _Name, _IsVarArg, _ReturnType, ...)                     \
   {                                                                            \
     SmallVector<Type *, 8> ArgsTypes({__VA_ARGS__});                           \
     Function *F = M.getFunction(_Name);                                        \
     if (declMatchesRTFTypes(F, _ReturnType, ArgsTypes)) {                      \
       auto &RFI = RFIs[_Enum];                                                 \
       RFI.Kind = _Enum;                                                        \
       RFI.Name = _Name;                                                        \
       RFI.IsVarArg = _IsVarArg;                                                \
       RFI.ReturnType = _ReturnType;                                            \
       RFI.ArgumentTypes = std::move(ArgsTypes);                                \
       RFI.Declaration = F;                                                     \
       unsigned NumUses = CollectUses(RFI);                                     \
       (void)NumUses;                                                           \
       LLVM_DEBUG({                                                             \
         dbgs() << TAG << RFI.Name << (RFI.Declaration ? "" : " not")           \
                << " found\n";                                                  \
         if (RFI.Declaration)                                                   \
           dbgs() << TAG << "-> got " << NumUses << " uses in "                 \
                  << RFI.getNumFunctionsWithUses()                              \
                  << " different functions.\n";                                 \
       });                                                                      \
     }                                                                          \
   }
 #include "llvm/Frontend/OpenMP/OMPKinds.def"
 
   // TODO: We should attach the attributes defined in OMPKinds.def.
 }
 
 bool OMPInformationCache::declMatchesRTFTypes(
     Function *F, Type *RTFRetType, SmallVector<Type *, 8> &RTFArgTypes) {
   // TODO: We should output information to the user (under debug output
   //       and via remarks).
 
   if (!F)
     return false;
   if (F->getReturnType() != RTFRetType)
     return false;
   if (F->arg_size() != RTFArgTypes.size())
     return false;
 
   auto RTFTyIt = RTFArgTypes.begin();
   for (Argument &Arg : F->args()) {
     if (Arg.getType() != *RTFTyIt)
       return false;
 
     ++RTFTyIt;
   }
 
   return true;
 }
 
 //===----------------------------------------------------------------------===//
 // Definitions of the MemoryTransfer helper structure.
 //===----------------------------------------------------------------------===//
 
 bool MemoryTransfer::getValuesInOffloadArrays() {
   // A runtime call that involves memory offloading looks something like:
   // call void @__tgt_target_data_begin(arg0, arg1,
   //   i8** %offload_baseptrs, i8** %offload_ptrs, i64* %offload_sizes,
   // ...)
   // So, the idea is to access the allocas that allocate space for these offload
   // arrays, offload_baseptrs, offload_ptrs, offload_sizes.
   // Therefore:
   // i8** %offload_baseptrs.
   const unsigned BasePtrsArgNum = 2;
   Use *BasePtrsArg = RuntimeCall->arg_begin() + BasePtrsArgNum;
   // i8** %offload_ptrs.
   const unsigned PtrsArgNum = 3;
   Use *PtrsArg = RuntimeCall->arg_begin() + PtrsArgNum;
   // i8** %offload_sizes.
   const unsigned SizesArgNum = 4;
   Use *SizesArg = RuntimeCall->arg_begin() + SizesArgNum;
 
   const DataLayout &DL = InfoCache.getDL();
 
   // Get values stored in **offload_baseptrs.
   auto *V = GetUnderlyingObject(BasePtrsArg->get(), DL);
   if (!isa<AllocaInst>(V)) {
     LLVM_DEBUG(dbgs() << TAG << "Couldn't get offload_baseptrs, only"
                       << " alloca arrays supported. In call to "
                       << RuntimeCall->getName() << " in function "
                       << RuntimeCall->getCaller()->getName() << "\n");
     return false;
   }
 
   auto *Array = cast<AllocaInst>(V);
   BasePtrs = OffloadArray::initialize(*Array, *RuntimeCall, InfoCache);
   if (!BasePtrs) {
     LLVM_DEBUG(dbgs() << TAG << "Couldn't get offload_baseptrs in call to "
                       << RuntimeCall->getName() << " in function "
                       << RuntimeCall->getCaller()->getName() << "\n");
     return false;
   }
 
   // Get values stored in **offload_ptrs.
   V = GetUnderlyingObject(PtrsArg->get(), DL);
   if (!isa<AllocaInst>(V)) {
     LLVM_DEBUG(dbgs() << TAG << "Couldn't get offload_ptrs, only"
                       << " alloca arrays supported. In call to "
                       << RuntimeCall->getName() << " in function "
                       << RuntimeCall->getCaller()->getName() << "\n");
     return false;
   }
   Array = cast<AllocaInst>(V);
   Ptrs = OffloadArray::initialize(*Array, *RuntimeCall, InfoCache);
   if (!Ptrs) {
     LLVM_DEBUG(dbgs() << TAG << "Couldn't get offload_ptrs in call to "
                       << RuntimeCall->getName() << " in function "
                       << RuntimeCall->getCaller()->getName() << "\n");
     return false;
   }
 
   // Get values stored in **offload_sizes.
   V = GetUnderlyingObject(SizesArg->get(), DL);
   // Sometimes the frontend generates this array as a constant global array.
   if (!isa<GlobalValue>(V)) {
     if (!isa<AllocaInst>(V)) {
       LLVM_DEBUG(dbgs() << TAG << "Couldn't get offload_sizes, only"
                         << " alloca arrays supported. In call to "
                         << RuntimeCall->getName() << " in function "
                         << RuntimeCall->getCaller()->getName() << "\n");
       return false;
     }
 
     Array = cast<AllocaInst>(V);
     Sizes = OffloadArray::initialize(*Array, *RuntimeCall, InfoCache);
     if (!Sizes) {
       LLVM_DEBUG(dbgs() << TAG << "Couldn't get offload_sizes in call to "
                         << RuntimeCall->getName() << " in function "
                         << RuntimeCall->getCaller()->getName() << "\n");
       return false;
     }
   }
 
   return true;
 }
 
 bool MemoryTransfer::detectIssue() {
   assert(BasePtrs && Ptrs && "No offload arrays to look at!");
 
   bool Success = getSetupInstructions(BasePtrs);
   if (!Success) {
     LLVM_DEBUG(dbgs() << TAG << "Couldn't get setup instructions of "
                       << "offload_baseptrs. In call to "
                       << RuntimeCall->getName() << " in function "
                       << RuntimeCall->getCaller()->getName() << "\n");
     return false;
   }
+  auto *BasePtrsGEP =
+      cast<Instruction>(RuntimeCall->getArgOperand(BasePtrsArgNum));
+  Issue.insert(BasePtrsGEP);
 
   Success = getSetupInstructions(Ptrs);
   if (!Success) {
     LLVM_DEBUG(dbgs() << TAG << "Couldn't get setup instructions of "
                       << "offload_ptrs. In call to "
                       << RuntimeCall->getName() << " in function "
                       << RuntimeCall->getCaller()->getName() << "\n");
     return false;
   }
+  auto *PtrsGEP =
+      cast<Instruction>(RuntimeCall->getArgOperand(PtrsArgNum));
+  Issue.insert(PtrsGEP);
 
   if (Sizes) {
     Success = getSetupInstructions(Sizes);
     if (!Success) {
       LLVM_DEBUG(dbgs() << TAG << "Couldn't get setup instructions of "
                         << "offload_sizes. In call to "
                         << RuntimeCall->getName() << " in function "
                         << RuntimeCall->getCaller()->getName() << "\n");
       return false;
     }
+    auto *SizesGEP =
+        cast<Instruction>(RuntimeCall->getArgOperand(SizesArgNum));
+    Issue.insert(SizesGEP);
   }
 
   return true;
 }
 
 bool MemoryTransfer::getSetupInstructions(std::unique_ptr<OffloadArray> &OA) {
   for (auto *S : OA->LastAccesses) {
     if (!getValueSetupInstructions(S))
       return false;
 
     if (!getPointerSetupInstructions(S))
       return false;
 
     Issue.insert(S);
   }
   return true;
 }
 
 bool MemoryTransfer::getPointerSetupInstructions(StoreInst *S) {
   auto *P = S->getPointerOperand();
 
   // TODO: P might be a global value. Make it general.
   if (!isa<Instruction>(P))
     return false;
 
   auto *DstInst = cast<Instruction>(P);
   if (isa<GetElementPtrInst>(DstInst)) {
     Issue.insert(DstInst);
 
   } else if (DstInst->isCast()) {
     auto *Casted = DstInst->getOperand(0);
 
     // TODO: Casted might be a global value. Make it general.
     if (!isa<Instruction>(Casted))
       return false;
 
     if (auto *GEP = dyn_cast<GetElementPtrInst>(Casted))
       Issue.insert(GEP);
 
     Issue.insert(DstInst);
   }
 
   return true;
 }
 
 bool MemoryTransfer::getValueSetupInstructions(StoreInst *S) {
   auto *V = S->getValueOperand();
   // Auxiliary storage to later insert the found instructions in the order
   // needed.
   SmallVector<Instruction *, 8> TempStorage;
   bool Success = false;
   unsigned MaxLookup = 6;
   for (unsigned I = 0; I < MaxLookup; ++I) {
     if (isa<AllocaInst>(V) || isa<Argument>(V) || isa<GlobalValue>(V) ||
         isa<Constant>(V)) {
       Success = true;
       break;
     }
 
     if (!isa<Instruction>(V)) {
       Success = false;
       break;
     }
 
     auto *Inst = cast<Instruction>(V);
     TempStorage.push_back(Inst);
 
     // FIXME: Inst might depend on more instructions through its second operand.
     V = Inst->getOperand(0);
   }
 
   if (Success)
     while (!TempStorage.empty())
       Issue.insert(TempStorage.pop_back_val());
 
   return Success;
 }
 
 bool MemoryTransfer::mayBeModifiedBy(Instruction *I) {
   assert(BasePtrs && Ptrs && "No offload addresses to analyze!");
   if (Issue.count(I))
     return false;
 
   if (mayModify(I, BasePtrs->StoredValues))
     return true;
   if (mayModify(I, Ptrs->StoredValues))
     return true;
   if (Sizes) {
     if (mayModify(I, Sizes->StoredValues))
       return true;
   }
 
   return false;
 }
 
 bool MemoryTransfer::mayModify(Instruction *I,
                                SmallVectorImpl<Value *> &Values) {
   assert(I && "Can't analyze nullptr!");
   auto *AAResults = InfoCache.getAnalysisResultForFunction<AAManager>(
       *RuntimeCall->getCaller());
   if (!AAResults) {
     LLVM_DEBUG(dbgs() << TAG << "Couldn't get AAManager in function "
                       << RuntimeCall->getCaller()->getName() << "\n");
     return true;
   }
 
   const DataLayout &DL = InfoCache.getDL();
 
   if (isa<StoreInst>(I)) {
     auto *Dst = GetUnderlyingObject(I->getOperand(1), DL);
     for (auto *V : Values) {
       if (Dst == V) {
         return true;
       }
     }
   } else if (isa<CallInst>(I)) {
     for (auto *V : Values) {
       // FIXME: This usage of the AAResults is not working properly. It always
       //        returns that the call instruction I may modify a value V.
       //        For example:
       //        define i32 @func(double* noalias %a) {
       //        ...
       //        %1 = call i32 @rand()
       //        ...
       //        }
       //        The getModRefInfo always returns that rand() modifies %a, even
       //        though it has the noalias attribute.
       auto ModRefResult = AAResults->getModRefInfo(
           I, MemoryLocation(V, LocationSize::precise(
                                    V->getType()->getPrimitiveSizeInBits()))
           );
       if (isModSet(ModRefResult))
         return true;
     }
   }
 
   return true;
 }
 
+bool MemoryTransfer::split(Instruction *After, Instruction *Before) {
+  assert((After || Before) &&
+         "Must have a place to move the split runtime call");
+
+  auto *M = RuntimeCall->getModule();
+  auto &IRBuilder = InfoCache.OMPBuilder;
+  // Add "issue" runtime call declaration.
+  // declare %struct.tgt_async_info @__tgt_target_data_begin_issue(i64, i32,
+  //   i8**, i8**, i64*, i64*)
+  FunctionCallee IssueDecl = IRBuilder.getOrCreateRuntimeFunction(
+      *M, OMPRTL___tgt_target_data_begin_issue);
+
+  // Change RuntimeCall callsite for its asynchronous version.
+  std::vector<Value *> Args;
+  Args.reserve(RuntimeCall->getNumArgOperands());
+  for (auto &Arg : RuntimeCall->args())
+    Args.push_back(Arg.get());
+
+  CallInst *IssueCallsite = CallInst::Create(
+      IssueDecl, ArrayRef<Value *>(Args), "handle", RuntimeCall);
+  RuntimeCall->removeFromParent();
+  RuntimeCall->deleteValue();
+  Issue.insert(IssueCallsite);
+
+  // Add "wait" runtime call declaration.
+  // declare void @__tgt_target_data_begin_wait(i64, %struct.__tgt_async_info)
+  FunctionCallee WaitDecl = IRBuilder.getOrCreateRuntimeFunction(
+      *M, OMPRTL___tgt_target_data_begin_wait);
+
+  // Add "wait" call site.
+  const unsigned WaitNumParams = 2;
+  Value *WaitParams[] = {
+      IssueCallsite->getArgOperand(0), // device_id.
+      IssueCallsite // returned handle.
+  };
+  CallInst *WaitCallsite = CallInst::Create(
+      WaitDecl, ArrayRef<Value*>(WaitParams, WaitNumParams), /*NameStr=*/"",
+      /*InsertBefore=*/(Instruction *)nullptr);
+
+  // Move wait.
+  if (!Before)
+    WaitCallsite->insertAfter(IssueCallsite);
+  else
+    WaitCallsite->insertBefore(Before);
+
+  if (After)
+    moveIssue(After);
+
+  return true;
+}
+
+void MemoryTransfer::moveIssue(Instruction *After) {
+  Instruction *Before = After->getNextNode();
+  for (auto *I : Issue) {
+    I->removeFromParent();
+    I->insertBefore(Before);
+  }
+}
+
 std::unique_ptr<OffloadArray> OffloadArray::initialize(
     AllocaInst &Array, Instruction &Before, InformationCache &InfoCache) {
   if (!Array.getAllocatedType()->isArrayTy()) {
     LLVM_DEBUG(dbgs() << TAG << "Allocated type is not array.\n");
     return nullptr;
   }
 
   auto OA = std::make_unique<OffloadArray>(Array, InfoCache);
   bool Success = OA->getValues(Before);
   if (!Success) {
     LLVM_DEBUG(dbgs() << TAG << "Error getting values in offload array.\n");
     return nullptr;
   }
 
   return OA;
 }
 
 bool OffloadArray::getValues(Instruction &Before) {
   // Initialize container.
   const uint64_t NumValues =
       Array.getAllocatedType()->getArrayNumElements();
   StoredValues.assign(NumValues, nullptr);
   LastAccesses.assign(NumValues, nullptr);
 
   // TODO: This assumes the instruction \p Before is in the same BasicBlock
   //       as OffloadArray::Array. Make it general, for any control flow graph.
   auto *BB = Array.getParent();
   if (BB != Before.getParent()) {
     LLVM_DEBUG(dbgs() << TAG << "The lower limit instruction is in a"
                       << " different BasicBlock.\n");
     return false;
   }
 
   const DataLayout &DL = InfoCache.getDL();
   for (auto &I : *BB) {
     if (&I == &Before) break;
 
     if (auto *S = dyn_cast<StoreInst>(&I)) {
       auto *Dst = GetUnderlyingObject(S->getPointerOperand(), DL);
 
       if (Dst == &Array) {
         int32_t Idx = getAccessedIdx(*S);
         if (Idx < 0) {
           LLVM_DEBUG(dbgs() << TAG << "Unexpected StoreInst\n");
           return false;
         }
 
         StoredValues[Idx] = GetUnderlyingObject(S->getValueOperand(), DL);
         LastAccesses[Idx] = S;
       }
     }
   }
 
   return isFilled();
 }
 
 int32_t OffloadArray::getAccessedIdx(StoreInst &S) {
   auto *Dst = S.getOperand(1);
   if (!isa<Instruction>(Dst)) {
     LLVM_DEBUG(dbgs() << TAG << "Unrecognized store pattern.\n");
     return -1;
   }
   auto *DstInst = cast<Instruction>(Dst);
 
   Value *Access = DstInst;
   if (DstInst->isCast()) {
     Access = DstInst->getOperand(0);
 
     // Direct cast from the AllocaInst, which means a store to the
     // first position of the array.
     if (Access == &Array) return 0;
   }
 
   if (!isa<GetElementPtrInst>(Access)) {
     LLVM_DEBUG(dbgs() << TAG << "Unrecognized store pattern.\n");
     return -1;
   }
   auto *GEPInst = cast<GetElementPtrInst>(Access);
 
   auto *ArrayIdx = GEPInst->idx_begin() + 1;
   if (ArrayIdx == GEPInst->idx_end()) {
     LLVM_DEBUG(dbgs() << TAG << "Unrecognized store pattern.\n");
     return -1;
   }
 
   return OpenMPOpt::getIntLiteral(ArrayIdx->get());
 }
 
 bool OffloadArray::isFilled() {
   const unsigned NumValues = StoredValues.size();
   for (unsigned I = 0; I < NumValues; ++I) {
     if (!StoredValues[I] || !LastAccesses[I])
       return false;
   }
 
   return true;
 }
 
 //===----------------------------------------------------------------------===//
 // Declarations and definitions of AAICVTracker.
 //===----------------------------------------------------------------------===//
 namespace {
 
 /// Abstract Attribute for tracking ICV values.
 struct AAICVTracker : public StateWrapper<BooleanState, AbstractAttribute> {
   using Base = StateWrapper<BooleanState, AbstractAttribute>;
   AAICVTracker(const IRPosition &IRP, Attributor &A) : Base(IRP) {}
 
   /// Returns true if value is assumed to be tracked.
   bool isAssumedTracked() const { return getAssumed(); }
 
   /// Returns true if value is known to be tracked.
   bool isKnownTracked() const { return getAssumed(); }
 
   /// Create an abstract attribute biew for the position \p IRP.
   static AAICVTracker &createForPosition(const IRPosition &IRP, Attributor &A);
 
   /// Return the value with which \p I can be replaced for specific \p ICV.
   virtual Value *getReplacementValue(InternalControlVar ICV,
                                      const Instruction *I, Attributor &A) = 0;
 
   /// See AbstractAttribute::getName()
   const std::string getName() const override { return "AAICVTracker"; }
 
   static const char ID;
 };
 
 struct AAICVTrackerFunction : public AAICVTracker {
   AAICVTrackerFunction(const IRPosition &IRP, Attributor &A)
       : AAICVTracker(IRP, A) {}
 
   // FIXME: come up with better string.
   const std::string getAsStr() const override { return "ICVTracker"; }
 
   // FIXME: come up with some stats.
   void trackStatistics() const override {}
 
   /// TODO: decide whether to deduplicate here, or use current
   /// deduplicateRuntimeCalls function.
   ChangeStatus manifest(Attributor &A) override {
     ChangeStatus Changed = ChangeStatus::UNCHANGED;
 
     for (InternalControlVar &ICV : TrackableICVs)
       if (deduplicateICVGetters(ICV, A))
         Changed = ChangeStatus::CHANGED;
 
     return Changed;
   }
 
   bool deduplicateICVGetters(InternalControlVar &ICV, Attributor &A) {
     auto &OMPInfoCache = static_cast<OMPInformationCache &>(A.getInfoCache());
     auto &ICVInfo = OMPInfoCache.ICVs[ICV];
     auto &GetterRFI = OMPInfoCache.RFIs[ICVInfo.Getter];
 
     bool Changed = false;
 
     auto ReplaceAndDeleteCB = [&](Use &U, Function &Caller) {
       CallInst *CI = OpenMPOpt::getCallIfRegularCall(U, &GetterRFI);
       Instruction *UserI = cast<Instruction>(U.getUser());
       Value *ReplVal = getReplacementValue(ICV, UserI, A);
 
       if (!ReplVal || !CI)
         return false;
 
       A.removeCallSite(CI);
       CI->replaceAllUsesWith(ReplVal);
       CI->eraseFromParent();
       Changed = true;
       return true;
     };
 
     GetterRFI.foreachUse(ReplaceAndDeleteCB);
     return Changed;
   }
 
   // Map of ICV to their values at specific program point.
   EnumeratedArray<SmallSetVector<ICVValue, 4>, InternalControlVar,
       InternalControlVar::ICV___last>
       ICVValuesMap;
 
   // Currently only nthreads is being tracked.
   // this array will only grow with time.
   InternalControlVar TrackableICVs[1] = {ICV_nthreads};
 
   ChangeStatus updateImpl(Attributor &A) override {
     ChangeStatus HasChanged = ChangeStatus::UNCHANGED;
 
     Function *F = getAnchorScope();
 
     auto &OMPInfoCache = static_cast<OMPInformationCache &>(A.getInfoCache());
 
     for (InternalControlVar ICV : TrackableICVs) {
       auto &SetterRFI = OMPInfoCache.RFIs[OMPInfoCache.ICVs[ICV].Setter];
 
       auto TrackValues = [&](Use &U, Function &) {
         CallInst *CI = OpenMPOpt::getCallIfRegularCall(U);
         if (!CI)
           return false;
 
         // FIXME: handle setters with more that 1 arguments.
         /// Track new value.
         if (ICVValuesMap[ICV].insert(ICVValue(CI, CI->getArgOperand(0))))
           HasChanged = ChangeStatus::CHANGED;
 
         return false;
       };
 
       SetterRFI.foreachUse(TrackValues, F);
     }
 
     return HasChanged;
   }
 
   /// Return the value with which \p I can be replaced for specific \p ICV.
   Value *getReplacementValue(InternalControlVar ICV, const Instruction *I,
                              Attributor &A) override {
     const BasicBlock *CurrBB = I->getParent();
 
     auto &ValuesSet = ICVValuesMap[ICV];
     auto &OMPInfoCache = static_cast<OMPInformationCache &>(A.getInfoCache());
     auto &GetterRFI = OMPInfoCache.RFIs[OMPInfoCache.ICVs[ICV].Getter];
 
     for (const auto &ICVVal : ValuesSet) {
       if (CurrBB == ICVVal.Inst->getParent()) {
         if (!ICVVal.Inst->comesBefore(I))
           continue;
 
         // both instructions are in the same BB and at \p I we know the ICV
         // value.
         while (I != ICVVal.Inst) {
           // we don't yet know if a call might update an ICV.
           // TODO: check callsite AA for value.
           if (const auto *CB = dyn_cast<CallBase>(I))
             if (CB->getCalledFunction() != GetterRFI.Declaration)
               return nullptr;
 
           I = I->getPrevNode();
         }
 
         // No call in between, return the value.
         return ICVVal.TrackedValue;
       }
     }
 
     // No value was tracked.
     return nullptr;
   }
 };
 } // namespace
 
 const char AAICVTracker::ID = 0;
 
 AAICVTracker &AAICVTracker::createForPosition(const IRPosition &IRP,
                                               Attributor &A) {
   AAICVTracker *AA = nullptr;
   switch (IRP.getPositionKind()) {
   case IRPosition::IRP_INVALID:
   case IRPosition::IRP_FLOAT:
   case IRPosition::IRP_ARGUMENT:
   case IRPosition::IRP_RETURNED:
   case IRPosition::IRP_CALL_SITE_RETURNED:
   case IRPosition::IRP_CALL_SITE_ARGUMENT:
   case IRPosition::IRP_CALL_SITE:
     llvm_unreachable("ICVTracker can only be created for function position!");
   case IRPosition::IRP_FUNCTION:
     AA = new (A.Allocator) AAICVTrackerFunction(IRP, A);
     break;
   }
 
   return *AA;
 }
 
 //===----------------------------------------------------------------------===//
 // Definitions of the OpenMPOpt structure.
 //===----------------------------------------------------------------------===//
 
 bool OpenMPOpt::run() {
   bool Changed = false;
 
   LLVM_DEBUG(dbgs() << TAG << "Run on SCC with " << SCC.size()
                     << " functions in a slice with "
                     << OMPInfoCache.ModuleSlice.size() << " functions\n");
 
   /// Print initial ICV values for testing.
   /// FIXME: This should be done from the Attributor once it is added.
   if (PrintICVValues) {
     InternalControlVar ICVs[] = {ICV_nthreads, ICV_active_levels, ICV_cancel};
 
     for (Function *F : OMPInfoCache.ModuleSlice) {
       for (auto ICV : ICVs) {
         auto ICVInfo = OMPInfoCache.ICVs[ICV];
         auto Remark = [&](OptimizationRemark OR) {
           return OR << "OpenMP ICV " << ore::NV("OpenMPICV", ICVInfo.Name)
                     << " Value: "
                     << (ICVInfo.InitValue
                         ? ICVInfo.InitValue->getValue().toString(10, true)
                         : "IMPLEMENTATION_DEFINED");
         };
 
         emitRemarkOnFunction(F, "OpenMPICVTracker", Remark);
       }
     }
   }
 
   Changed |= runAttributor();
   Changed |= deduplicateRuntimeCalls();
   Changed |= deleteParallelRegions();
-  Changed |= hideMemTransfersLatency();
+  if (SplitMemoryTransfers)
+    Changed |= hideMemTransfersLatency();
 
   return Changed;
 }
 
 CallInst *OpenMPOpt::getCallIfRegularCall(
     Use &U, OMPInformationCache::RuntimeFunctionInfo *RFI) {
   CallInst *CI = dyn_cast<CallInst>(U.getUser());
   if (CI && CI->isCallee(&U) && !CI->hasOperandBundles() &&
       (!RFI || CI->getCalledFunction() == RFI->Declaration))
     return CI;
   return nullptr;
 }
 
 CallInst *OpenMPOpt::getCallIfRegularCall(
     Value &V, OMPInformationCache::RuntimeFunctionInfo *RFI) {
   CallInst *CI = dyn_cast<CallInst>(&V);
   if (CI && !CI->hasOperandBundles() &&
       (!RFI || CI->getCalledFunction() == RFI->Declaration))
     return CI;
   return nullptr;
 }
 
 bool OpenMPOpt::deleteParallelRegions() {
   const unsigned CallbackCalleeOperand = 2;
 
   OMPInformationCache::RuntimeFunctionInfo &RFI =
       OMPInfoCache.RFIs[OMPRTL___kmpc_fork_call];
 
   if (!RFI.Declaration)
     return false;
 
   bool Changed = false;
   auto DeleteCallCB = [&](Use &U, Function &) {
     CallInst *CI = getCallIfRegularCall(U);
     if (!CI)
       return false;
     auto *Fn = dyn_cast<Function>(
         CI->getArgOperand(CallbackCalleeOperand)->stripPointerCasts());
     if (!Fn)
       return false;
     if (!Fn->onlyReadsMemory())
       return false;
     if (!Fn->hasFnAttribute(Attribute::WillReturn))
       return false;
 
     LLVM_DEBUG(dbgs() << TAG << "Delete read-only parallel region in "
                       << CI->getCaller()->getName() << "\n");
 
     auto Remark = [&](OptimizationRemark OR) {
       return OR << "Parallel region in "
                 << ore::NV("OpenMPParallelDelete", CI->getCaller()->getName())
                 << " deleted";
     };
     emitRemark<OptimizationRemark>(CI, "OpenMPParallelRegionDeletion",
                                    Remark);
 
     CGUpdater.removeCallSite(*CI);
     CI->eraseFromParent();
     Changed = true;
     ++NumOpenMPParallelRegionsDeleted;
     return true;
   };
 
   RFI.foreachUse(DeleteCallCB);
 
   return Changed;
 }
 
 bool OpenMPOpt::deduplicateRuntimeCalls() {
   bool Changed = false;
 
   RuntimeFunction DeduplicableRuntimeCallIDs[] = {
       OMPRTL_omp_get_num_threads,
       OMPRTL_omp_in_parallel,
       OMPRTL_omp_get_cancellation,
       OMPRTL_omp_get_thread_limit,
       OMPRTL_omp_get_supported_active_levels,
       OMPRTL_omp_get_level,
       OMPRTL_omp_get_ancestor_thread_num,
       OMPRTL_omp_get_team_size,
       OMPRTL_omp_get_active_level,
       OMPRTL_omp_in_final,
       OMPRTL_omp_get_proc_bind,
       OMPRTL_omp_get_num_places,
       OMPRTL_omp_get_num_procs,
       OMPRTL_omp_get_place_num,
       OMPRTL_omp_get_partition_num_places,
       OMPRTL_omp_get_partition_place_nums};
 
   // Global-tid is handled separately.
   SmallSetVector<Value *, 16> GTIdArgs;
   collectGlobalThreadIdArguments(GTIdArgs);
   LLVM_DEBUG(dbgs() << TAG << "Found " << GTIdArgs.size()
                     << " global thread ID arguments\n");
 
   for (Function *F : SCC) {
     for (auto DeduplicableRuntimeCallID : DeduplicableRuntimeCallIDs)
       deduplicateRuntimeCalls(*F,
                               OMPInfoCache.RFIs[DeduplicableRuntimeCallID]);
 
     // __kmpc_global_thread_num is special as we can replace it with an
     // argument in enough cases to make it worth trying.
     Value *GTIdArg = nullptr;
     for (Argument &Arg : F->args())
       if (GTIdArgs.count(&Arg)) {
         GTIdArg = &Arg;
         break;
       }
     Changed |= deduplicateRuntimeCalls(
         *F, OMPInfoCache.RFIs[OMPRTL___kmpc_global_thread_num], GTIdArg);
   }
 
   return Changed;
 }
 
 bool OpenMPOpt::hideMemTransfersLatency() {
   OMPInformationCache::RuntimeFunctionInfo &RFI =
       OMPInfoCache.RFIs[OMPRTL___tgt_target_data_begin];
 
   bool Changed = false;
   auto SplitDataTransfer = [&] (Use &U, Function &Decl) {
     auto *RTCall = getCallIfRegularCall(U, &RFI);
     if (!RTCall)
       return false;
 
     MemoryTransfer MT(RTCall, OMPInfoCache);
     bool Success = MT.getValuesInOffloadArrays();
     if (!Success) {
       LLVM_DEBUG(dbgs() << TAG << "Couldn't get offload arrays in call to "
                         << MT.RuntimeCall->getName() << " in function "
                         << MT.RuntimeCall->getCaller()->getName() << "\n");
       return false;
     }
 
     Success = MT.detectIssue();
     if (!Success) {
       LLVM_DEBUG(dbgs() << TAG << "Couldn't detect issue in call to "
                         << MT.RuntimeCall->getName() << " in function "
                         << MT.RuntimeCall->getCaller()->getName() << "\n");
       return false;
     }
 
-    if (canBeMovedUpwards(MT) || canBeMovedDownwards(MT)) {
-      // TODO: Split runtime call.
-    }
-
-    return false;
+    auto *After = canBeMovedUpwards(MT);
+    auto *Before = canBeMovedDownwards(MT);
+    return (After || Before) && MT.split(After, Before);
   };
 
   RFI.foreachUse(SplitDataTransfer);
   return Changed;
 }
 
 Instruction *OpenMPOpt::canBeMovedUpwards(MemoryTransfer &MT) {
   assert(MT.Issue.size() > 0 && "There's not set of instructions to be moved!");
 
   CallInst *RC = MT.RuntimeCall;
   auto *MSSAResult =
       OMPInfoCache.getAnalysisResultForFunction<MemorySSAAnalysis>(
           *RC->getCaller());
   if (!MSSAResult) {
     LLVM_DEBUG(dbgs() << TAG << "Couldn't get MemorySSAAnalysis in function "
                       << RC->getCaller()->getName() << "\n");
     return nullptr;
   }
 
   auto &MSSA = MSSAResult->getMSSA();
   auto *MSSAWalker = MSSA.getWalker();
   const auto *LiveOnEntry = MSSA.getLiveOnEntryDef();
   auto *MemAccess = MSSAWalker->getClobberingMemoryAccess(RC);
 
   while (MemAccess != LiveOnEntry) {
     if (!isa<MemoryDef>(MemAccess))
       continue;
 
     auto *MemInst = (cast<MemoryDef>(MemAccess))->getMemoryInst();
     if (MT.mayBeModifiedBy(MemInst)) {
       // If MemInst is not the instruction immediately before the Issue.
       if (!MT.Issue.count(MemInst->getNextNode()))
         return MemInst;
 
       return nullptr;
     }
 
     MemAccess = MSSAWalker->getClobberingMemoryAccess(MemAccess);
   }
 
   return nullptr;
 }
 
 Instruction *OpenMPOpt::canBeMovedDownwards(MemoryTransfer &MT) {
   assert(MT.Issue.size() > 0 && "There's not set of instructions to be moved!");
 
   // FIXME: This traverses only the BasicBlock where MT is. Make it traverse
   //        the CFG.
   GlobalValue *TgtTargetDecl = M.getNamedValue("__tgt_target");
   GlobalValue *TgtTargetTeamsDecl = M.getNamedValue("__tgt_target_teams");
   GlobalValue *TgtTargetDataEndDecl = M.getNamedValue("__tgt_target_data_end");
   CallInst *RC = MT.RuntimeCall;
   auto *I = RC->getNextNode();
   while (I) {
     if (auto *C = dyn_cast<CallInst>(I)) {
       auto *Callee = C->getCalledFunction();
       if (Callee == TgtTargetDecl)
         return I;
       if (Callee == TgtTargetTeamsDecl)
         return I;
       if (Callee == TgtTargetDataEndDecl)
         return I;
     }
 
     I = I->getNextNode();
   }
 
   // Return end of BasicBlock.
   return &*(RC->getParent()->end());
 }
 
 Value *OpenMPOpt::combinedIdentStruct(Value *CurrentIdent, Value *NextIdent,
     bool GlobalOnly, bool &SingleChoice) {
   if (CurrentIdent == NextIdent)
     return CurrentIdent;
 
   // TODO: Figure out how to actually combine multiple debug locations. For
   //       now we just keep an existing one if there is a single choice.
   if (!GlobalOnly || isa<GlobalValue>(NextIdent)) {
     SingleChoice = !CurrentIdent;
     return NextIdent;
   }
   return nullptr;
 }
 
 Value * OpenMPOpt::getCombinedIdentFromCallUsesIn(
     OMPInformationCache::RuntimeFunctionInfo &RFI,
     Function &F, bool GlobalOnly) {
   bool SingleChoice = true;
   Value *Ident = nullptr;
   auto CombineIdentStruct = [&](Use &U, Function &Caller) {
     CallInst *CI = getCallIfRegularCall(U, &RFI);
     if (!CI || &F != &Caller)
       return false;
     Ident = combinedIdentStruct(Ident, CI->getArgOperand(0),
         /* GlobalOnly */ true, SingleChoice);
     return false;
   };
   RFI.foreachUse(CombineIdentStruct);
 
   if (!Ident || !SingleChoice) {
     // The IRBuilder uses the insertion block to get to the module, this is
     // unfortunate but we work around it for now.
     if (!OMPInfoCache.OMPBuilder.getInsertionPoint().getBlock())
       OMPInfoCache.OMPBuilder.updateToLocation(OpenMPIRBuilder::InsertPointTy(
           &F.getEntryBlock(), F.getEntryBlock().begin()));
     // Create a fallback location if non was found.
     // TODO: Use the debug locations of the calls instead.
     Constant *Loc = OMPInfoCache.OMPBuilder.getOrCreateDefaultSrcLocStr();
     Ident = OMPInfoCache.OMPBuilder.getOrCreateIdent(Loc);
   }
   return Ident;
 }
 
 bool OpenMPOpt::deduplicateRuntimeCalls(
     Function &F, OMPInformationCache::RuntimeFunctionInfo &RFI,
     Value *ReplVal) {
   auto *UV = RFI.getUseVector(F);
   if (!UV || UV->size() + (ReplVal != nullptr) < 2)
     return false;
 
   LLVM_DEBUG(
       dbgs() << TAG << "Deduplicate " << UV->size() << " uses of " << RFI.Name
              << (ReplVal ? " with an existing value\n" : "\n") << "\n");
 
   assert((!ReplVal || (isa<Argument>(ReplVal) &&
                        cast<Argument>(ReplVal)->getParent() == &F)) &&
          "Unexpected replacement value!");
 
   // TODO: Use dominance to find a good position instead.
   auto CanBeMoved = [](CallBase &CB) {
     unsigned NumArgs = CB.getNumArgOperands();
     if (NumArgs == 0)
       return true;
     if (CB.getArgOperand(0)->getType() != IdentPtr)
       return false;
     for (unsigned u = 1; u < NumArgs; ++u)
       if (isa<Instruction>(CB.getArgOperand(u)))
         return false;
     return true;
   };
 
   if (!ReplVal) {
     for (Use *U : *UV)
       if (CallInst *CI = getCallIfRegularCall(*U, &RFI)) {
         if (!CanBeMoved(*CI))
           continue;
 
         auto Remark = [&](OptimizationRemark OR) {
           auto newLoc = &*F.getEntryBlock().getFirstInsertionPt();
           return OR << "OpenMP runtime call "
                     << ore::NV("OpenMPOptRuntime", RFI.Name) << " moved to "
                     << ore::NV("OpenMPRuntimeMoves", newLoc->getDebugLoc());
         };
         emitRemark<OptimizationRemark>(CI, "OpenMPRuntimeCodeMotion", Remark);
 
         CI->moveBefore(&*F.getEntryBlock().getFirstInsertionPt());
         ReplVal = CI;
         break;
       }
     if (!ReplVal)
       return false;
   }
 
   // If we use a call as a replacement value we need to make sure the ident is
   // valid at the new location. For now we just pick a global one, either
   // existing and used by one of the calls, or created from scratch.
   if (CallBase *CI = dyn_cast<CallBase>(ReplVal)) {
     if (CI->getNumArgOperands() > 0 &&
         CI->getArgOperand(0)->getType() == IdentPtr) {
       Value *Ident = getCombinedIdentFromCallUsesIn(RFI, F,
           /* GlobalOnly */ true);
       CI->setArgOperand(0, Ident);
     }
   }
 
   bool Changed = false;
   auto ReplaceAndDeleteCB = [&](Use &U, Function &Caller) {
     CallInst *CI = getCallIfRegularCall(U, &RFI);
     if (!CI || CI == ReplVal || &F != &Caller)
       return false;
     assert(CI->getCaller() == &F && "Unexpected call!");
 
     auto Remark = [&](OptimizationRemark OR) {
       return OR << "OpenMP runtime call "
                 << ore::NV("OpenMPOptRuntime", RFI.Name) << " deduplicated";
     };
     emitRemark<OptimizationRemark>(CI, "OpenMPRuntimeDeduplicated", Remark);
 
     CGUpdater.removeCallSite(*CI);
     CI->replaceAllUsesWith(ReplVal);
     CI->eraseFromParent();
     ++NumOpenMPRuntimeCallsDeduplicated;
     Changed = true;
     return true;
   };
   RFI.foreachUse(ReplaceAndDeleteCB);
 
   return Changed;
 }
 
 void OpenMPOpt::collectGlobalThreadIdArguments(
     SmallSetVector<Value *, 16> &GTIdArgs) {
   // TODO: Below we basically perform a fixpoint iteration with a pessimistic
   //       initialization. We could define an AbstractAttribute instead and
   //       run the Attributor here once it can be run as an SCC pass.
 
   // Helper to check the argument \p ArgNo at all call sites of \p F for
   // a GTId.
   auto CallArgOpIsGTId = [&](Function &F, unsigned ArgNo, CallInst &RefCI) {
     if (!F.hasLocalLinkage())
       return false;
     for (Use &U : F.uses()) {
       if (CallInst *CI = getCallIfRegularCall(U)) {
         Value *ArgOp = CI->getArgOperand(ArgNo);
         if (CI == &RefCI || GTIdArgs.count(ArgOp) ||
             getCallIfRegularCall(
                 *ArgOp, &OMPInfoCache.RFIs[OMPRTL___kmpc_global_thread_num]))
           continue;
       }
       return false;
     }
     return true;
   };
 
   // Helper to identify uses of a GTId as GTId arguments.
   auto AddUserArgs = [&](Value &GTId) {
     for (Use &U : GTId.uses())
       if (CallInst *CI = dyn_cast<CallInst>(U.getUser()))
         if (CI->isArgOperand(&U))
           if (Function *Callee = CI->getCalledFunction())
             if (CallArgOpIsGTId(*Callee, U.getOperandNo(), *CI))
               GTIdArgs.insert(Callee->getArg(U.getOperandNo()));
   };
 
   // The argument users of __kmpc_global_thread_num calls are GTIds.
   OMPInformationCache::RuntimeFunctionInfo &GlobThreadNumRFI =
       OMPInfoCache.RFIs[OMPRTL___kmpc_global_thread_num];
 
   GlobThreadNumRFI.foreachUse([&](Use &U, Function &F) {
     if (CallInst *CI =
         getCallIfRegularCall(U, &GlobThreadNumRFI))
       AddUserArgs(*CI);
     return false;
   });
 
   // Transitively search for more arguments by looking at the users of the
   // ones we know already. During the search the GTIdArgs vector is extended
   // so we cannot cache the size nor can we use a range based for.
   for (unsigned u = 0; u < GTIdArgs.size(); ++u)
     AddUserArgs(*GTIdArgs[u]);
 }
 
 template <typename RemarkKind, typename RemarkCallBack>
 void OpenMPOpt::emitRemark(Instruction *Inst, StringRef RemarkName,
                            RemarkCallBack &&RemarkCB) {
   Function *F = Inst->getParent()->getParent();
   auto &ORE = OREGetter(F);
 
   ORE.emit(
       [&]() { return RemarkCB(RemarkKind(DEBUG_TYPE, RemarkName, Inst)); });
 }
 
 void OpenMPOpt::emitRemarkOnFunction(
     Function *F, StringRef RemarkName,
     function_ref<OptimizationRemark(OptimizationRemark &&)> &&RemarkCB) {
   auto &ORE = OREGetter(F);
 
   ORE.emit([&]() {
     return RemarkCB(OptimizationRemark(DEBUG_TYPE, RemarkName, F));
   });
 }
 
 bool OpenMPOpt::runAttributor() {
   if (SCC.empty())
     return false;
 
   registerAAs();
 
   ChangeStatus Changed = A.run();
 
   LLVM_DEBUG(dbgs() << "[Attributor] Done with " << SCC.size()
                     << " functions, result: " << Changed << ".\n");
 
   return Changed == ChangeStatus::CHANGED;
 }
 
 void OpenMPOpt::registerAAs() {
   for (Function *F : SCC) {
     if (F->isDeclaration())
       continue;
 
     A.getOrCreateAAFor<AAICVTracker>(IRPosition::function(*F));
   }
 }
 
 //===----------------------------------------------------------------------===//
 // Definitions of the OpenMPOptPass.
 //===----------------------------------------------------------------------===//
 
 PreservedAnalyses OpenMPOptPass::run(LazyCallGraph::SCC &C,
                                      CGSCCAnalysisManager &AM,
                                      LazyCallGraph &CG, CGSCCUpdateResult &UR) {
   if (!containsOpenMP(*C.begin()->getFunction().getParent(), OMPInModule))
     return PreservedAnalyses::all();
 
   if (DisableOpenMPOptimizations)
     return PreservedAnalyses::all();
 
   SmallPtrSet<Function *, 16> ModuleSlice;
   SmallVector<Function *, 16> SCC;
   for (LazyCallGraph::Node &N : C) {
     SCC.push_back(&N.getFunction());
     ModuleSlice.insert(SCC.back());
   }
 
   if (SCC.empty())
     return PreservedAnalyses::all();
 
   FunctionAnalysisManager &FAM =
       AM.getResult<FunctionAnalysisManagerCGSCCProxy>(C, CG).getManager();
 
   AnalysisGetter AG(FAM);
 
   auto OREGetter = [&FAM](Function *F) -> OptimizationRemarkEmitter & {
     return FAM.getResult<OptimizationRemarkEmitterAnalysis>(*F);
   };
 
   CallGraphUpdater CGUpdater;
   CGUpdater.initialize(CG, C, AM, UR);
 
   SetVector<Function *> Functions(SCC.begin(), SCC.end());
   BumpPtrAllocator Allocator;
   OMPInformationCache InfoCache(*(Functions.back()->getParent()), AG, Allocator,
                                 /*CGSCC*/ &Functions, ModuleSlice);
 
   Attributor A(Functions, InfoCache, CGUpdater);
 
   // TODO: Compute the module slice we are allowed to look at.
   OpenMPOpt OMPOpt(SCC, CGUpdater, OREGetter, InfoCache, A);
   bool Changed = OMPOpt.run();
   (void)Changed;
   return PreservedAnalyses::all();
 }
 
 namespace {
 
 struct OpenMPOptLegacyPass : public CallGraphSCCPass {
   CallGraphUpdater CGUpdater;
   OpenMPInModule OMPInModule;
   static char ID;
 
   OpenMPOptLegacyPass() : CallGraphSCCPass(ID) {
     initializeOpenMPOptLegacyPassPass(*PassRegistry::getPassRegistry());
   }
 
   void getAnalysisUsage(AnalysisUsage &AU) const override {
     CallGraphSCCPass::getAnalysisUsage(AU);
   }
 
   bool doInitialization(CallGraph &CG) override {
     // Disable the pass if there is no OpenMP (runtime call) in the module.
     containsOpenMP(CG.getModule(), OMPInModule);
     return false;
   }
 
   bool runOnSCC(CallGraphSCC &CGSCC) override {
     if (!containsOpenMP(CGSCC.getCallGraph().getModule(), OMPInModule))
       return false;
     if (DisableOpenMPOptimizations || skipSCC(CGSCC))
       return false;
 
     SmallPtrSet<Function *, 16> ModuleSlice;
     SmallVector<Function *, 16> SCC;
     for (CallGraphNode *CGN : CGSCC)
       if (Function *Fn = CGN->getFunction())
         if (!Fn->isDeclaration()) {
           SCC.push_back(Fn);
           ModuleSlice.insert(Fn);
         }
 
     if (SCC.empty())
       return false;
 
     CallGraph &CG = getAnalysis<CallGraphWrapperPass>().getCallGraph();
     CGUpdater.initialize(CG, CGSCC);
 
     // Maintain a map of functions to avoid rebuilding the ORE
     DenseMap<Function *, std::unique_ptr<OptimizationRemarkEmitter>> OREMap;
     auto OREGetter = [&OREMap](Function *F) -> OptimizationRemarkEmitter & {
       std::unique_ptr<OptimizationRemarkEmitter> &ORE = OREMap[F];
       if (!ORE)
         ORE = std::make_unique<OptimizationRemarkEmitter>(F);
       return *ORE;
     };
 
     AnalysisGetter AG;
     SetVector<Function *> Functions(SCC.begin(), SCC.end());
     BumpPtrAllocator Allocator;
     OMPInformationCache InfoCache(*(Functions.back()->getParent()), AG,
                                   Allocator,
                                   /*CGSCC*/ &Functions, ModuleSlice);
 
     Attributor A(Functions, InfoCache, CGUpdater);
 
     // TODO: Compute the module slice we are allowed to look at.
     OpenMPOpt OMPOpt(SCC, CGUpdater, OREGetter, InfoCache, A);
     return OMPOpt.run();
   }
 
   bool doFinalization(CallGraph &CG) override { return CGUpdater.finalize(); }
 };
 
 } // end anonymous namespace
 
 bool llvm::omp::containsOpenMP(Module &M, OpenMPInModule &OMPInModule) {
   if (OMPInModule.isKnown())
     return OMPInModule;
 
 #define OMP_RTL(_Enum, _Name, ...)                                             \
   if (M.getFunction(_Name))                                                    \
     return OMPInModule = true;
 #include "llvm/Frontend/OpenMP/OMPKinds.def"
   return OMPInModule = false;
 }
 
 char OpenMPOptLegacyPass::ID = 0;
 
 INITIALIZE_PASS_BEGIN(OpenMPOptLegacyPass, "openmpopt",
                       "OpenMP specific optimizations", false, false)
 INITIALIZE_PASS_DEPENDENCY(CallGraphWrapperPass)
 INITIALIZE_PASS_END(OpenMPOptLegacyPass, "openmpopt",
                     "OpenMP specific optimizations", false, false)
 
 Pass *llvm::createOpenMPOptLegacyPass() { return new OpenMPOptLegacyPass(); }
diff --git a/llvm/test/Transforms/OpenMP/hide_mem_transfer_latency.ll b/llvm/test/Transforms/OpenMP/hide_mem_transfer_latency.ll
index daec0e5e78d..b0d750e7d0e 100644
--- a/llvm/test/Transforms/OpenMP/hide_mem_transfer_latency.ll
+++ b/llvm/test/Transforms/OpenMP/hide_mem_transfer_latency.ll
@@ -1,561 +1,523 @@
-; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: -p --function-signature
-; RUN: opt -S -passes=openmpopt < %s | FileCheck %s
+; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --function-signature --scrub-attributes
+; RUN: opt -S -passes=openmpopt -aa-pipeline=basic-aa -openmp-split-memtransfers < %s | FileCheck %s
 target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"

-; FIXME: This struct should be generated after splitting at least one of the runtime calls.
-; %struct.__tgt_async_info = type { i8* }
+; CHECK: %struct.__tgt_async_info = type { i8* }
 %struct.ident_t = type { i32, i32, i32, i32, i8* }
 %struct.__tgt_offload_entry = type { i8*, i8*, i64, i32, i32 }

 @.offload_maptypes = private unnamed_addr constant [1 x i64] [i64 35]
 @.__omp_offloading_heavyComputation1.region_id = weak constant i8 0
 @.offload_sizes.1 = private unnamed_addr constant [1 x i64] [i64 8]
 @.offload_maptypes.2 = private unnamed_addr constant [1 x i64] [i64 800]

 @.__omp_offloading_heavyComputation2.region_id = weak constant i8 0
 @.offload_maptypes.3 = private unnamed_addr constant [2 x i64] [i64 35, i64 35]

 @.__omp_offloading_heavyComputation3.region_id = weak constant i8 0
 @.offload_sizes.2 = private unnamed_addr constant [2 x i64] [i64 4, i64 0]
 @.offload_maptypes.4 = private unnamed_addr constant [2 x i64] [i64 800, i64 544]

 @.offload_maptypes.5 = private unnamed_addr constant [1 x i64] [i64 33]

 ;double heavyComputation1() {
 ;  double a = rand() % 777;
 ;  double random = rand();
 ;
 ;  //#pragma omp target data map(a)
 ;  void* args[1];
 ;  args[0] = &a;
 ;  __tgt_target_data_begin(..., args, ...)
 ;
 ;  #pragma omp target teams
 ;  for (int i = 0; i < 1000; ++i) {
 ;    a *= i*i / 2;
 ;  }
 ;
 ;  return random + a;
 ;}
 define dso_local double @heavyComputation1() {
 ; CHECK-LABEL: define {{[^@]+}}@heavyComputation1()
 ; CHECK-NEXT:  entry:
 ; CHECK-NEXT:    %a = alloca double, align 8
 ; CHECK-NEXT:    %.offload_baseptrs = alloca [1 x i8*], align 8
 ; CHECK-NEXT:    %.offload_ptrs = alloca [1 x i8*], align 8
 ; CHECK-NEXT:    %.offload_baseptrs4 = alloca [1 x i8*], align 8
 ; CHECK-NEXT:    %.offload_ptrs5 = alloca [1 x i8*], align 8
 ; CHECK-NEXT:    %0 = bitcast double* %a to i8*
 ; CHECK-NEXT:    %call = call i32 @rand()
 ; CHECK-NEXT:    %rem = srem i32 %call, 777
 ; CHECK-NEXT:    %conv = sitofp i32 %rem to double
 ; CHECK-NEXT:    store double %conv, double* %a, align 8
+
 ; CHECK-NEXT:    %call1 = call i32 @rand()
+
 ; CHECK-NEXT:    %1 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs, i64 0, i64 0
 ; CHECK-NEXT:    %2 = bitcast [1 x i8*]* %.offload_baseptrs to double**
 ; CHECK-NEXT:    store double* %a, double** %2, align 8
 ; CHECK-NEXT:    %3 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs, i64 0, i64 0
 ; CHECK-NEXT:    %4 = bitcast [1 x i8*]* %.offload_ptrs to double**
 ; CHECK-NEXT:    store double* %a, double** %4, align 8
-; CHECK-NEXT:    call void @__tgt_target_data_begin(i64 -1, i32 1, i8** nonnull %1, i8** nonnull %3, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.1, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes, i64 0, i64 0))
+
+; CHECK-NEXT:    %handle = call %struct.__tgt_async_info @__tgt_target_data_begin_issue(i64 -1, i32 1, i8** %1, i8** %3, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.1, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes, i64 0, i64 0))
+
 ; CHECK-NEXT:    %5 = bitcast double* %a to i64*
 ; CHECK-NEXT:    %6 = load i64, i64* %5, align 8
 ; CHECK-NEXT:    %7 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs4, i64 0, i64 0
 ; CHECK-NEXT:    %8 = bitcast [1 x i8*]* %.offload_baseptrs4 to i64*
 ; CHECK-NEXT:    store i64 %6, i64* %8, align 8
 ; CHECK-NEXT:    %9 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs5, i64 0, i64 0
 ; CHECK-NEXT:    %10 = bitcast [1 x i8*]* %.offload_ptrs5 to i64*
 ; CHECK-NEXT:    store i64 %6, i64* %10, align 8
-; CHECK-NEXT:    %11 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_heavyComputation1.region_id, i32 1, i8** nonnull %7, i8** nonnull %9, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.1, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.2, i64 0, i64 0), i32 0, i32 0)
+
+; CHECK-NEXT:    call void @__tgt_target_data_begin_wait(i64 -1, %struct.__tgt_async_info %handle)
+
+; CHECK-NEXT:    %11 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_heavyComputation1.region_id, i32 1, i8** nocapture %7, i8** nocapture %9, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.1, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.2, i64 0, i64 0), i32 0, i32 0)
 ; CHECK-NEXT:    %12 = icmp eq i32 %11, 0
 ; CHECK-NEXT:    br i1 %12, label %omp_offload.cont, label %omp_offload.failed
 ; CHECK:       omp_offload.failed:
 ; CHECK-NEXT:    call void @heavyComputation1FallBack(i64 %6)
 ; CHECK-NEXT:    br label %omp_offload.cont
 ; CHECK:       omp_offload.cont:
 ; CHECK-NEXT:    %conv2 = sitofp i32 %call1 to double
 ; CHECK-NEXT:    call void @__tgt_target_data_end(i64 -1, i32 1, i8** nonnull %1, i8** nonnull %3, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.1, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes, i64 0, i64 0))
 ; CHECK-NEXT:    %13 = load double, double* %a, align 8
 ; CHECK-NEXT:    %add = fadd double %13, %conv2
 ; CHECK-NEXT:    ret double %add
 ;
 entry:
   %a = alloca double, align 8
   %.offload_baseptrs = alloca [1 x i8*], align 8
   %.offload_ptrs = alloca [1 x i8*], align 8
   %.offload_baseptrs4 = alloca [1 x i8*], align 8
   %.offload_ptrs5 = alloca [1 x i8*], align 8

-  ; FIXME: Should have after splitting the runtime call __tgt_target_data_begin.
-  ; %device_id1 = alloca i64, align 8
-  ; %async_info1 = alloca %struct.__tgt_async_info, align 8
-
   %0 = bitcast double* %a to i8*
   %call = call i32 @rand()
   %rem = srem i32 %call, 777
   %conv = sitofp i32 %rem to double
   store double %conv, double* %a, align 8

-  ; FIXME: The "isue" should be moved here.
+  ; FIXME: call i8* @__tgt_target_data_begin_issue(...) should be moved here.
   %call1 = call i32 @rand()

-  ; FIXME: This setup for the runtime call __tgt_target_data_begin should be
-  ;        split into its "issue" and "wait" counterpars and moved upwards
-  ;        and downwards, respectively. The call should be replaced to something
-  ;        like ...
-  ; Issue - this is moved upwards.
-  ; ... setup code ...
-  ; store i64 -1, i64* %device_id1, align 8
-  ; %handle1 = call i8* @__tgt_target_data_begin(i64* dereferenceable(8) %device_id1, i32 1, i8** nonnull %1, i8** nonnull %3, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.1, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes, i64 0, i64 0))
-  ; Wait - this is moved downwards.
-  ; %device_id1_copy = load i64, i64* %device_id1, align 8 ; device_id
-  ; %queue1 = getelementptr inbounds %struct.__tgt_async_info, %struct.__tgt_async_info* %async_info1, i32 0, i32 0
-  ; store i8* %handle1, i8** %queue1, align 8
-  ; call void @__tgt_target_data_begin_wait(i64 %device_id1_copy, %struct.__tgt_async_info* dereferenceable(8) %async_info1)
   %1 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs, i64 0, i64 0
   %2 = bitcast [1 x i8*]* %.offload_baseptrs to double**
   store double* %a, double** %2, align 8
   %3 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs, i64 0, i64 0
   %4 = bitcast [1 x i8*]* %.offload_ptrs to double**
   store double* %a, double** %4, align 8
   call void @__tgt_target_data_begin(i64 -1, i32 1, i8** nonnull %1, i8** nonnull %3, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.1, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes, i64 0, i64 0))

   %5 = bitcast double* %a to i64*
   %6 = load i64, i64* %5, align 8
   %7 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs4, i64 0, i64 0
   %8 = bitcast [1 x i8*]* %.offload_baseptrs4 to i64*
   store i64 %6, i64* %8, align 8
   %9 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs5, i64 0, i64 0
   %10 = bitcast [1 x i8*]* %.offload_ptrs5 to i64*
   store i64 %6, i64* %10, align 8

-  ; FIXME: The "wait" should be moved here.
-  %11 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_heavyComputation1.region_id, i32 1, i8** nonnull %7, i8** nonnull %9, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.1, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.2, i64 0, i64 0), i32 0, i32 0)
+  %11 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_heavyComputation1.region_id, i32 1, i8** nocapture %7, i8** nocapture %9, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.1, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.2, i64 0, i64 0), i32 0, i32 0)

   %12 = icmp eq i32 %11, 0
   br i1 %12, label %omp_offload.cont, label %omp_offload.failed

 omp_offload.failed:                               ; preds = %entry
   call void @heavyComputation1FallBack(i64 %6)
   br label %omp_offload.cont

 omp_offload.cont:                                 ; preds = %entry, %omp_offload.failed
   %conv2 = sitofp i32 %call1 to double
   call void @__tgt_target_data_end(i64 -1, i32 1, i8** nonnull %1, i8** nonnull %3, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.1, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes, i64 0, i64 0))
   %13 = load double, double* %a, align 8
   %add = fadd double %13, %conv2
   ret double %add
 }

 define internal void @heavyComputation1FallBack(i64 %a) {
+; CHECK-LABEL: define {{[^@]+}}@heavyComputation1FallBack(i64 %a)
+; CHECK-NEXT:  entry:
+; CHECK-NEXT:    ret void
+;
 entry:
   ; Fallback for offloading function heavyComputation1.
   ret void
 }

 ;int heavyComputation2(double* a, unsigned size) {
 ;  int random = rand() % 7;
 ;
 ;  //#pragma omp target data map(a[0:size], size)
 ;  void* args[2];
 ;  args[0] = &a;
 ;  args[1] = &size;
 ;  __tgt_target_data_begin(..., args, ...)
 ;
 ;  #pragma omp target teams
 ;  for (int i = 0; i < size; ++i) {
 ;    a[i] = ++a[i] * 3.141624;
 ;  }
 ;
 ;  return random;
 ;}
 define dso_local i32 @heavyComputation2(double* %a, i32 %size) {
 ; CHECK-LABEL: define {{[^@]+}}@heavyComputation2(double* %a, i32 %size)
 ; CHECK-NEXT:  entry:
 ; CHECK-NEXT:    %size.addr = alloca i32, align 4
 ; CHECK-NEXT:    %.offload_baseptrs = alloca [2 x i8*], align 8
 ; CHECK-NEXT:    %.offload_ptrs = alloca [2 x i8*], align 8
 ; CHECK-NEXT:    %.offload_sizes = alloca [2 x i64], align 8
 ; CHECK-NEXT:    %.offload_baseptrs2 = alloca [2 x i8*], align 8
 ; CHECK-NEXT:    %.offload_ptrs3 = alloca [2 x i8*], align 8
 ; CHECK-NEXT:    store i32 %size, i32* %size.addr, align 4
+
 ; CHECK-NEXT:    %call = call i32 @rand()
+
 ; CHECK-NEXT:    %conv = zext i32 %size to i64
 ; CHECK-NEXT:    %0 = shl nuw nsw i64 %conv, 3
 ; CHECK-NEXT:    %1 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs, i64 0, i64 0
 ; CHECK-NEXT:    %2 = bitcast [2 x i8*]* %.offload_baseptrs to double**
 ; CHECK-NEXT:    store double* %a, double** %2, align 8
 ; CHECK-NEXT:    %3 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs, i64 0, i64 0
 ; CHECK-NEXT:    %4 = bitcast [2 x i8*]* %.offload_ptrs to double**
 ; CHECK-NEXT:    store double* %a, double** %4, align 8
 ; CHECK-NEXT:    %5 = getelementptr inbounds [2 x i64], [2 x i64]* %.offload_sizes, i64 0, i64 0
 ; CHECK-NEXT:    store i64 %0, i64* %5, align 8
 ; CHECK-NEXT:    %6 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs, i64 0, i64 1
 ; CHECK-NEXT:    %7 = bitcast i8** %6 to i32**
 ; CHECK-NEXT:    store i32* %size.addr, i32** %7, align 8
 ; CHECK-NEXT:    %8 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs, i64 0, i64 1
 ; CHECK-NEXT:    %9 = bitcast i8** %8 to i32**
 ; CHECK-NEXT:    store i32* %size.addr, i32** %9, align 8
 ; CHECK-NEXT:    %10 = getelementptr inbounds [2 x i64], [2 x i64]* %.offload_sizes, i64 0, i64 1
 ; CHECK-NEXT:    store i64 4, i64* %10, align 8
-; CHECK-NEXT:    call void @__tgt_target_data_begin(i64 -1, i32 2, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))
+; CHECK-NEXT:    %handle = call %struct.__tgt_async_info @__tgt_target_data_begin_issue(i64 -1, i32 2, i8** %1, i8** %3, i64* %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))
+
 ; CHECK-NEXT:    %11 = load i32, i32* %size.addr, align 4
 ; CHECK-NEXT:    %size.casted = zext i32 %11 to i64
 ; CHECK-NEXT:    %12 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs2, i64 0, i64 0
 ; CHECK-NEXT:    %13 = bitcast [2 x i8*]* %.offload_baseptrs2 to i64*
 ; CHECK-NEXT:    store i64 %size.casted, i64* %13, align 8
 ; CHECK-NEXT:    %14 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs3, i64 0, i64 0
 ; CHECK-NEXT:    %15 = bitcast [2 x i8*]* %.offload_ptrs3 to i64*
 ; CHECK-NEXT:    store i64 %size.casted, i64* %15, align 8
 ; CHECK-NEXT:    %16 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs2, i64 0, i64 1
 ; CHECK-NEXT:    %17 = bitcast i8** %16 to double**
 ; CHECK-NEXT:    store double* %a, double** %17, align 8
 ; CHECK-NEXT:    %18 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs3, i64 0, i64 1
 ; CHECK-NEXT:    %19 = bitcast i8** %18 to double**
 ; CHECK-NEXT:    store double* %a, double** %19, align 8
+
+; CHECK-NEXT:    call void @__tgt_target_data_begin_wait(i64 -1, %struct.__tgt_async_info %handle)
+
 ; CHECK-NEXT:    %20 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_heavyComputation2.region_id, i32 2, i8** nonnull %12, i8** nonnull %14, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_sizes.2, i64 0, i64 0), i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.4, i64 0, i64 0), i32 0, i32 0)
 ; CHECK-NEXT:    %21 = icmp eq i32 %20, 0
 ; CHECK-NEXT:    br i1 %21, label %omp_offload.cont, label %omp_offload.failed
 ; CHECK:       omp_offload.failed:
 ; CHECK-NEXT:    call void @heavyComputation2FallBack(i64 %size.casted, double* %a)
 ; CHECK-NEXT:    br label %omp_offload.cont
 ; CHECK:       omp_offload.cont:
 ; CHECK-NEXT:    %rem = srem i32 %call, 7
 ; CHECK-NEXT:    call void @__tgt_target_data_end(i64 -1, i32 2, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))
 ; CHECK-NEXT:    ret i32 %rem
 ;
 entry:
   %size.addr = alloca i32, align 4
   %.offload_baseptrs = alloca [2 x i8*], align 8
   %.offload_ptrs = alloca [2 x i8*], align 8
   %.offload_sizes = alloca [2 x i64], align 8
   %.offload_baseptrs2 = alloca [2 x i8*], align 8
   %.offload_ptrs3 = alloca [2 x i8*], align 8

-  ; FIXME: Should have after splitting the runtime call __tgt_target_data_begin.
-  ; %device_id1 = alloca i64, align 8
-  ; %async_info1 = alloca %struct.__tgt_async_info, align 8
-
   store i32 %size, i32* %size.addr, align 4
   %call = call i32 @rand()

-  ; FIXME: This setup for the runtime call __tgt_target_data_begin should be
-  ;        split into its "issue" and "wait" counterpars. Here though, the "issue"
-  ;        cannot be moved upwards because it's not guaranteed that rand()
-  ;        won't modify *a. Nevertheless, the "wait" can be moved downwards.
-  ;        The call should be replaced to something like ...
-  ; Issue - this can't be moved upwards, *a might have aliases.
-  ; ... setup code ...
-  ; store i64 -1, i64* %device_id1, align 8
-  ; %handle1 = call i8* @__tgt_target_data_begin(i64* dereferenceable(8) %device_id1, i32 2, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))
-  ; Wait - this is moved downards.
-  ; %device_id1_copy = load i64, i64* %device_id1, align 8 ; device_id
-  ; %queue1 = getelementptr inbounds %struct.__tgt_async_info, %struct.__tgt_async_info* %async_info1, i32 0, i32 0
-  ; store i8* %handle1, i8** %queue1, align 8
-  ; call void @__tgt_target_data_begin_wait(i64 %device_id1_copy, %struct.__tgt_async_info* dereferenceable(8) %async_info1)
   %conv = zext i32 %size to i64
   %0 = shl nuw nsw i64 %conv, 3
   %1 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs, i64 0, i64 0
   %2 = bitcast [2 x i8*]* %.offload_baseptrs to double**
   store double* %a, double** %2, align 8
   %3 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs, i64 0, i64 0
   %4 = bitcast [2 x i8*]* %.offload_ptrs to double**
   store double* %a, double** %4, align 8
   %5 = getelementptr inbounds [2 x i64], [2 x i64]* %.offload_sizes, i64 0, i64 0
   store i64 %0, i64* %5, align 8
   %6 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs, i64 0, i64 1
   %7 = bitcast i8** %6 to i32**
   store i32* %size.addr, i32** %7, align 8
   %8 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs, i64 0, i64 1
   %9 = bitcast i8** %8 to i32**
   store i32* %size.addr, i32** %9, align 8
   %10 = getelementptr inbounds [2 x i64], [2 x i64]* %.offload_sizes, i64 0, i64 1
   store i64 4, i64* %10, align 8
   call void @__tgt_target_data_begin(i64 -1, i32 2, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))

   %11 = load i32, i32* %size.addr, align 4
   %size.casted = zext i32 %11 to i64
   %12 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs2, i64 0, i64 0
   %13 = bitcast [2 x i8*]* %.offload_baseptrs2 to i64*
   store i64 %size.casted, i64* %13, align 8
   %14 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs3, i64 0, i64 0
   %15 = bitcast [2 x i8*]* %.offload_ptrs3 to i64*
   store i64 %size.casted, i64* %15, align 8
   %16 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs2, i64 0, i64 1
   %17 = bitcast i8** %16 to double**
   store double* %a, double** %17, align 8
   %18 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs3, i64 0, i64 1
   %19 = bitcast i8** %18 to double**
   store double* %a, double** %19, align 8

-  ; FIXME: The "wait" should be moved here.
   %20 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_heavyComputation2.region_id, i32 2, i8** nonnull %12, i8** nonnull %14, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_sizes.2, i64 0, i64 0), i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.4, i64 0, i64 0), i32 0, i32 0)

   %21 = icmp eq i32 %20, 0
   br i1 %21, label %omp_offload.cont, label %omp_offload.failed

 omp_offload.failed:                               ; preds = %entry
   call void @heavyComputation2FallBack(i64 %size.casted, double* %a)
   br label %omp_offload.cont

 omp_offload.cont:                                 ; preds = %entry, %omp_offload.failed
   %rem = srem i32 %call, 7
   call void @__tgt_target_data_end(i64 -1, i32 2, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))
   ret i32 %rem
 }

 define internal void @heavyComputation2FallBack(i64 %size, double* %a) {
+; CHECK-LABEL: define {{[^@]+}}@heavyComputation2FallBack(i64 %size, double* %a)
+; CHECK-NEXT:  entry:
+; CHECK-NEXT:    ret void
+;
 entry:
   ; Fallback for offloading function heavyComputation2.
   ret void
 }

 ;int heavyComputation3(double* restrict a, unsigned size) {
 ;  int random = rand() % 7;
 ;
 ;  //#pragma omp target data map(a[0:size], size)
 ;  void* args[2];
 ;  args[0] = &a;
 ;  args[1] = &size;
 ;  __tgt_target_data_begin(..., args, ...)
 ;
 ;  #pragma omp target teams
 ;  for (int i = 0; i < size; ++i) {
 ;    a[i] = ++a[i] * 3.141624;
 ;  }
 ;
 ;  return random;
 ;}
 define dso_local i32 @heavyComputation3(double* noalias %a, i32 %size) {
 ; CHECK-LABEL: define {{[^@]+}}@heavyComputation3(double* noalias %a, i32 %size)
 ; CHECK-NEXT:  entry:
 ; CHECK-NEXT:    %size.addr = alloca i32, align 4
 ; CHECK-NEXT:    %.offload_baseptrs = alloca [2 x i8*], align 8
 ; CHECK-NEXT:    %.offload_ptrs = alloca [2 x i8*], align 8
 ; CHECK-NEXT:    %.offload_sizes = alloca [2 x i64], align 8
 ; CHECK-NEXT:    %.offload_baseptrs2 = alloca [2 x i8*], align 8
 ; CHECK-NEXT:    %.offload_ptrs3 = alloca [2 x i8*], align 8
 ; CHECK-NEXT:    store i32 %size, i32* %size.addr, align 4
+
 ; CHECK-NEXT:    %call = call i32 @rand()
+
 ; CHECK-NEXT:    %conv = zext i32 %size to i64
 ; CHECK-NEXT:    %0 = shl nuw nsw i64 %conv, 3
 ; CHECK-NEXT:    %1 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs, i64 0, i64 0
 ; CHECK-NEXT:    %2 = bitcast [2 x i8*]* %.offload_baseptrs to double**
 ; CHECK-NEXT:    store double* %a, double** %2, align 8
 ; CHECK-NEXT:    %3 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs, i64 0, i64 0
 ; CHECK-NEXT:    %4 = bitcast [2 x i8*]* %.offload_ptrs to double**
 ; CHECK-NEXT:    store double* %a, double** %4, align 8
 ; CHECK-NEXT:    %5 = getelementptr inbounds [2 x i64], [2 x i64]* %.offload_sizes, i64 0, i64 0
 ; CHECK-NEXT:    store i64 %0, i64* %5, align 8
 ; CHECK-NEXT:    %6 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs, i64 0, i64 1
 ; CHECK-NEXT:    %7 = bitcast i8** %6 to i32**
 ; CHECK-NEXT:    store i32* %size.addr, i32** %7, align 8
 ; CHECK-NEXT:    %8 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs, i64 0, i64 1
 ; CHECK-NEXT:    %9 = bitcast i8** %8 to i32**
 ; CHECK-NEXT:    store i32* %size.addr, i32** %9, align 8
 ; CHECK-NEXT:    %10 = getelementptr inbounds [2 x i64], [2 x i64]* %.offload_sizes, i64 0, i64 1
 ; CHECK-NEXT:    store i64 4, i64* %10, align 8
-; CHECK-NEXT:    call void @__tgt_target_data_begin(i64 -1, i32 2, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))
+; CHECK-NEXT:    %handle = call %struct.__tgt_async_info @__tgt_target_data_begin_issue(i64 -1, i32 2, i8** %1, i8** %3, i64* %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))
+
 ; CHECK-NEXT:    %11 = load i32, i32* %size.addr, align 4
 ; CHECK-NEXT:    %size.casted = zext i32 %11 to i64
 ; CHECK-NEXT:    %12 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs2, i64 0, i64 0
 ; CHECK-NEXT:    %13 = bitcast [2 x i8*]* %.offload_baseptrs2 to i64*
 ; CHECK-NEXT:    store i64 %size.casted, i64* %13, align 8
 ; CHECK-NEXT:    %14 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs3, i64 0, i64 0
 ; CHECK-NEXT:    %15 = bitcast [2 x i8*]* %.offload_ptrs3 to i64*
 ; CHECK-NEXT:    store i64 %size.casted, i64* %15, align 8
 ; CHECK-NEXT:    %16 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs2, i64 0, i64 1
 ; CHECK-NEXT:    %17 = bitcast i8** %16 to double**
 ; CHECK-NEXT:    store double* %a, double** %17, align 8
 ; CHECK-NEXT:    %18 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs3, i64 0, i64 1
 ; CHECK-NEXT:    %19 = bitcast i8** %18 to double**
 ; CHECK-NEXT:    store double* %a, double** %19, align 8
+
+; CHECK-NEXT:    call void @__tgt_target_data_begin_wait(i64 -1, %struct.__tgt_async_info %handle)
+
 ; CHECK-NEXT:    %20 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_heavyComputation3.region_id, i32 2, i8** nonnull %12, i8** nonnull %14, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_sizes.2, i64 0, i64 0), i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.4, i64 0, i64 0), i32 0, i32 0)
 ; CHECK-NEXT:    %21 = icmp eq i32 %20, 0
 ; CHECK-NEXT:    br i1 %21, label %omp_offload.cont, label %omp_offload.failed
 ; CHECK:       omp_offload.failed:
 ; CHECK-NEXT:    call void @heavyComputation3FallBack(i64 %size.casted, double* %a)
 ; CHECK-NEXT:    br label %omp_offload.cont
 ; CHECK:       omp_offload.cont:
 ; CHECK-NEXT:    %rem = srem i32 %call, 7
 ; CHECK-NEXT:    call void @__tgt_target_data_end(i64 -1, i32 2, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))
 ; CHECK-NEXT:    ret i32 %rem
 ;
 entry:
   %size.addr = alloca i32, align 4
   %.offload_baseptrs = alloca [2 x i8*], align 8
   %.offload_ptrs = alloca [2 x i8*], align 8
   %.offload_sizes = alloca [2 x i64], align 8
   %.offload_baseptrs2 = alloca [2 x i8*], align 8
   %.offload_ptrs3 = alloca [2 x i8*], align 8

-  ; FIXME: Should have after splitting the runtime call __tgt_target_data_begin.
-  ; %device_id1 = alloca i64, align 8
-  ; %async_info1 = alloca %struct.__tgt_async_info, align 8
-
   store i32 %size, i32* %size.addr, align 4

-  ; FIXME: The "issue" should be moved here.
+  ; FIXME: call i8* @__tgt_target_data_begin_issue(...) should be moved here.
   %call = call i32 @rand()

-  ; FIXME: This setup for the runtime call __tgt_target_data_begin should be
-  ;        split into its "issue" and "wait" counterpars and moved upwards
-  ;        and downwards, respectively. The call should be replaced to something
-  ;        like ...
-  ; Issue - this is moved upwards.
-  ; ... setup code ...
-  ; store i64 -1, i64* %device_id1, align 8
-  ; %handle1 = call i8* @__tgt_target_data_begin(i64* dereferenceable(8) %device_id1, i32 2, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))
-  ; Wait - this is moved downards.
-  ; %device_id1_copy = load i64, i64* %device_id1, align 8 ; device_id
-  ; %queue1 = getelementptr inbounds %struct.__tgt_async_info, %struct.__tgt_async_info* %async_info1, i32 0, i32 0
-  ; store i8* %handle1, i8** %queue1, align 8
-  ; call void @__tgt_target_data_begin_wait(i64 %device_id1_copy, %struct.__tgt_async_info* dereferenceable(8) %async_info1)
   %conv = zext i32 %size to i64
   %0 = shl nuw nsw i64 %conv, 3
   %1 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs, i64 0, i64 0
   %2 = bitcast [2 x i8*]* %.offload_baseptrs to double**
   store double* %a, double** %2, align 8
   %3 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs, i64 0, i64 0
   %4 = bitcast [2 x i8*]* %.offload_ptrs to double**
   store double* %a, double** %4, align 8
   %5 = getelementptr inbounds [2 x i64], [2 x i64]* %.offload_sizes, i64 0, i64 0
   store i64 %0, i64* %5, align 8
   %6 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs, i64 0, i64 1
   %7 = bitcast i8** %6 to i32**
   store i32* %size.addr, i32** %7, align 8
   %8 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs, i64 0, i64 1
   %9 = bitcast i8** %8 to i32**
   store i32* %size.addr, i32** %9, align 8
   %10 = getelementptr inbounds [2 x i64], [2 x i64]* %.offload_sizes, i64 0, i64 1
   store i64 4, i64* %10, align 8
   call void @__tgt_target_data_begin(i64 -1, i32 2, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))

   %11 = load i32, i32* %size.addr, align 4
   %size.casted = zext i32 %11 to i64
   %12 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs2, i64 0, i64 0
   %13 = bitcast [2 x i8*]* %.offload_baseptrs2 to i64*
   store i64 %size.casted, i64* %13, align 8
   %14 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs3, i64 0, i64 0
   %15 = bitcast [2 x i8*]* %.offload_ptrs3 to i64*
   store i64 %size.casted, i64* %15, align 8
   %16 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_baseptrs2, i64 0, i64 1
   %17 = bitcast i8** %16 to double**
   store double* %a, double** %17, align 8
   %18 = getelementptr inbounds [2 x i8*], [2 x i8*]* %.offload_ptrs3, i64 0, i64 1
   %19 = bitcast i8** %18 to double**
   store double* %a, double** %19, align 8

-  ; FIXME: The "wait" should be moved here.
   %20 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_heavyComputation3.region_id, i32 2, i8** nonnull %12, i8** nonnull %14, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_sizes.2, i64 0, i64 0), i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.4, i64 0, i64 0), i32 0, i32 0)

   %21 = icmp eq i32 %20, 0
   br i1 %21, label %omp_offload.cont, label %omp_offload.failed

 omp_offload.failed:                               ; preds = %entry
   call void @heavyComputation3FallBack(i64 %size.casted, double* %a)
   br label %omp_offload.cont

 omp_offload.cont:                                 ; preds = %entry, %omp_offload.failed
   %rem = srem i32 %call, 7
   call void @__tgt_target_data_end(i64 -1, i32 2, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([2 x i64], [2 x i64]* @.offload_maptypes.3, i64 0, i64 0))
   ret i32 %rem
 }

 define internal void @heavyComputation3FallBack(i64 %size, double* %a) {
+; CHECK-LABEL: define {{[^@]+}}@heavyComputation3FallBack(i64 %size, double* %a)
+; CHECK-NEXT:  entry:
+; CHECK-NEXT:    ret void
+;
 entry:
   ; Fallback for offloading function heavyComputation3.
   ret void
 }

 ;int dataTransferOnly1(double* restrict a, unsigned size) {
 ;  // Random computation.
 ;  int random = rand();
 ;
 ;  //#pragma omp target data map(to:a[0:size])
 ;  void* args[1];
 ;  args[0] = &a;
 ;  __tgt_target_data_begin(..., args, ...)
 ;
 ;  // Random computation.
 ;  random %= size;
 ;  return random;
 ;}
 define dso_local i32 @dataTransferOnly1(double* noalias %a, i32 %size) {
 ; CHECK-LABEL: define {{[^@]+}}@dataTransferOnly1(double* noalias %a, i32 %size)
 ; CHECK-NEXT:  entry:
 ; CHECK-NEXT:    %.offload_baseptrs = alloca [1 x i8*], align 8
 ; CHECK-NEXT:    %.offload_ptrs = alloca [1 x i8*], align 8
 ; CHECK-NEXT:    %.offload_sizes = alloca [1 x i64], align 8
+
 ; CHECK-NEXT:    %call = call i32 @rand()
+
 ; CHECK-NEXT:    %conv = zext i32 %size to i64
 ; CHECK-NEXT:    %0 = shl nuw nsw i64 %conv, 3
 ; CHECK-NEXT:    %1 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs, i64 0, i64 0
 ; CHECK-NEXT:    %2 = bitcast [1 x i8*]* %.offload_baseptrs to double**
 ; CHECK-NEXT:    store double* %a, double** %2, align 8
 ; CHECK-NEXT:    %3 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs, i64 0, i64 0
 ; CHECK-NEXT:    %4 = bitcast [1 x i8*]* %.offload_ptrs to double**
 ; CHECK-NEXT:    store double* %a, double** %4, align 8
 ; CHECK-NEXT:    %5 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes, i64 0, i64 0
 ; CHECK-NEXT:    store i64 %0, i64* %5, align 8
-; CHECK-NEXT:    call void @__tgt_target_data_begin(i64 -1, i32 1, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.5, i64 0, i64 0))
+; CHECK-NEXT:    %handle = call %struct.__tgt_async_info @__tgt_target_data_begin_issue(i64 -1, i32 1, i8** %1, i8** %3, i64* %5, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.5, i64 0, i64 0))
+
 ; CHECK-NEXT:    %rem = urem i32 %call, %size
+
+; CHECK-NEXT:    call void @__tgt_target_data_begin_wait(i64 -1, %struct.__tgt_async_info %handle)
+
 ; CHECK-NEXT:    call void @__tgt_target_data_end(i64 -1, i32 1, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.5, i64 0, i64 0))
 ; CHECK-NEXT:    ret i32 %rem
 ;
 entry:
   %.offload_baseptrs = alloca [1 x i8*], align 8
   %.offload_ptrs = alloca [1 x i8*], align 8
   %.offload_sizes = alloca [1 x i64], align 8

-  ; FIXME: Should have after splitting the runtime call __tgt_target_data_begin.
-  ; %device_id1 = alloca i64, align 8
-  ; %async_info1 = alloca %struct.__tgt_async_info, align 8
-
-  ; FIXME: The "issue" should be moved here.
+  ; FIXME: call i8* @__tgt_target_data_begin_issue(...) should be moved here.
   %call = call i32 @rand()

-  ; FIXME: This setup for the runtime call __tgt_target_data_begin should be
-  ;        split into its "issue" and "wait" counterpars and moved upwards
-  ;        and downwards, respectively. The call should be replaced to something
-  ;        like ...
-  ; Issue - this is moved upwards.
-  ; ... setup code ...
-  ; store i64 -1, i64* %device_id1, align 8
-  ; %handle1 = call i8* @__tgt_target_data_begin(i64* dereferenceable(8) %device_id1, i32 1, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.5, i64 0, i64 0))
-  ; Wait - this is moved downards.
-  ; %device_id1_copy = load i64, i64* %device_id1, align 8 ; device_id
-  ; %queue1 = getelementptr inbounds %struct.__tgt_async_info, %struct.__tgt_async_info* %async_info1, i32 0, i32 0
-  ; store i8* %handle1, i8** %queue1, align 8
-  ; call void @__tgt_target_data_begin_wait(i64 %device_id1_copy, %struct.__tgt_async_info* dereferenceable(8) %async_info1)
   %conv = zext i32 %size to i64
   %0 = shl nuw nsw i64 %conv, 3
   %1 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs, i64 0, i64 0
   %2 = bitcast [1 x i8*]* %.offload_baseptrs to double**
   store double* %a, double** %2, align 8
   %3 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs, i64 0, i64 0
   %4 = bitcast [1 x i8*]* %.offload_ptrs to double**
   store double* %a, double** %4, align 8
   %5 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes, i64 0, i64 0
   store i64 %0, i64* %5, align 8
   call void @__tgt_target_data_begin(i64 -1, i32 1, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.5, i64 0, i64 0))

   %rem = urem i32 %call, %size

-  ; FIXME: The "wait" should be moved here.
   call void @__tgt_target_data_end(i64 -1, i32 1, i8** nonnull %1, i8** nonnull %3, i64* nonnull %5, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.5, i64 0, i64 0))
   ret i32 %rem
 }

 declare dso_local void @__tgt_target_data_begin(i64, i32, i8**, i8**, i64*, i64*)

 declare dso_local i32 @__tgt_target_teams(i64, i8*, i32, i8**, i8**, i64*, i64*, i32, i32)

 declare dso_local void @__tgt_target_data_end(i64, i32, i8**, i8**, i64*, i64*)

 declare dso_local i32 @rand()

-; FIXME: These two function declarations must be generated after splitting the runtime function
-;        __tgt_target_data_begin.
-; declare dso_local i8* @__tgt_target_data_begin_issue(i64* dereferenceable(8), i32, i8**, i8**, i64*, i64*)
-; declare dso_local void @__tgt_target_data_begin_wait(i64, %struct.__tgt_async_info* dereferenceable(8))
+; CHECK: declare %struct.__tgt_async_info @__tgt_target_data_begin_issue(i64, i32, i8**, i8**, i64*, i64*)
+; CHECK: declare void @__tgt_target_data_begin_wait(i64, %struct.__tgt_async_info)
+